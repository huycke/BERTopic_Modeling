{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16b270e",
   "metadata": {},
   "source": [
    "## Notebook 01: Data Preprocessing\n",
    "\n",
    "This notebook uses the `load_and_preprocess_data` function from `src.preprocessing` to load raw data, apply cleaning and filtering, and save the processed output.\n",
    "\n",
    "**Target Data:** Semantic Scholar data (combining 'title' and 'abstract')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b54f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added c:\\WORKING\\BERTopic_Modeling to sys.path\n",
      "Successfully imported 'load_and_preprocess_data' from src.preprocessing\n"
     ]
    }
   ],
   "source": [
    "# ## 1. Imports and Setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# --- Add src directory to Python path ---\n",
    "# This allows importing modules from src. Adjust path if notebook is moved.\n",
    "module_path = os.path.abspath(os.path.join('..')) \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added {module_path} to sys.path\")\n",
    "else:\n",
    "    print(f\"{module_path} already in sys.path\")\n",
    "\n",
    "# --- Import the preprocessing function ---\n",
    "try:\n",
    "    from src.preprocessing import load_and_preprocess_data \n",
    "    print(\"Successfully imported 'load_and_preprocess_data' from src.preprocessing\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing functions: {e}\")\n",
    "    print(\"Ensure the 'src' directory is in the Python path and preprocessing.py exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during import: {e}\")\n",
    "\n",
    "# --- Configure Logging ---\n",
    "# Basic logging setup for notebook visibility\n",
    "# Use force=True to allow reconfiguring logging in Jupyter environment\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc4ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:39:56,814 - INFO - Project root identified as: c:\\WORKING\\BERTopic_Modeling\n",
      "2025-05-08 13:39:56,815 - INFO - Raw data file path: c:\\WORKING\\BERTopic_Modeling\\data\\raw\\reddit_allFoS_2015_to_2025_bulk_results.csv\n",
      "2025-05-08 13:39:56,816 - INFO - Processed data output file path: c:\\WORKING\\BERTopic_Modeling\\data\\processed\\s2_reddit_processed_docs.csv\n",
      "2025-05-08 13:39:56,818 - INFO - Configuration parameters defined for Semantic Scholar data.\n"
     ]
    }
   ],
   "source": [
    "# ## 2. Define Configuration for Semantic Scholar Data\n",
    "\n",
    "# --- Define paths ---\n",
    "project_root_dir = os.path.abspath(os.path.join('..')) \n",
    "raw_data_file = os.path.join(project_root_dir, 'data', 'raw', 'reddit_allFoS_2015_to_2025_bulk_results.csv')\n",
    "processed_data_output_file = os.path.join(project_root_dir, 'data', 'processed', 's2_processed_docs.csv')\n",
    "dropped_rows_file = os.path.join(project_root_dir, 'data', 'processed', 's2_dropped_missing_abstract.csv') # Path for rows dropped due to missing required columns\n",
    "\n",
    "logging.info(f\"Project root identified as: {project_root_dir}\")\n",
    "logging.info(f\"Raw data file path: {raw_data_file}\")\n",
    "logging.info(f\"Processed data output file path: {processed_data_output_file}\")\n",
    "logging.info(f\"Dropped rows output file path: {dropped_rows_file}\")\n",
    "\n",
    "# --- Parameters for load_and_preprocess_data --- \n",
    "param_file_path = raw_data_file\n",
    "param_text_source_columns = ['title', 'abstract']\n",
    "param_unique_id_column = 'corpusId' # Verify this matches your S2 CSV column name for unique paper ID\n",
    "\n",
    "# NEW: Specify columns required for 'docs' creation. Rows missing these will be dropped.\n",
    "param_required_columns_for_docs_creation = ['abstract'] # We want to drop if 'abstract' is missing/empty\n",
    "# NEW: Specify where to save these dropped rows\n",
    "param_dropped_rows_output_path = dropped_rows_file\n",
    "\n",
    "param_data_type_specific_df_processing = None \n",
    "param_clean_apply_unescape = True\n",
    "param_clean_apply_url_removal = True\n",
    "param_clean_apply_html_tag_removal = True\n",
    "param_clean_apply_quote_normalization = True\n",
    "param_clean_apply_char_filtering = True\n",
    "param_clean_char_filter_regex = r\"[^a-zA-Z0-9\\s,.!?':;\\\"-]\"\n",
    "param_clean_apply_html_entity_removal = True\n",
    "param_clean_apply_lowercase = True\n",
    "param_apply_length_filter = True\n",
    "param_min_doc_length = 50  \n",
    "param_max_doc_length = 10000 \n",
    "param_apply_duplicate_removal = True\n",
    "param_column_for_duplicate_checking = 'docs' \n",
    "param_apply_score_filter = False \n",
    "param_score_column_for_filtering = None \n",
    "param_min_score_for_filtering = None\n",
    "param_max_score_for_filtering = None\n",
    "\n",
    "logging.info(\"Configuration parameters defined for Semantic Scholar data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc2a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:40:00,930 - INFO - Attempting to load and preprocess data from: c:\\WORKING\\BERTopic_Modeling\\data\\raw\\reddit_allFoS_2015_to_2025_bulk_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for: c:\\WORKING\\BERTopic_Modeling\\data\\raw\\reddit_allFoS_2015_to_2025_bulk_results.csv\n",
      "Original dataset shape: (5446, 20)\n",
      "Created 'docs' column from: ['title', 'abstract']\n",
      "Using 'corpusId' as reference ID.\n",
      "Applying configurable text cleaning to 'docs' column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:40:01,232 - INFO - Preprocessing finished. Processed DataFrame shape: (4932, 21)\n",
      "2025-05-08 13:40:01,420 - INFO - Processed DataFrame saved to: c:\\WORKING\\BERTopic_Modeling\\data\\processed\\s2_reddit_processed_docs.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2 rows with empty 'docs' after text cleaning.\n",
      "Filtered by length (column: 'docs', min: 50, max: 10000): Removed 488 documents. Kept 4956 documents.\n",
      "Removed duplicates based on 'docs': Removed 24 documents. Kept 4932.\n",
      "Finished preprocessing. Processed dataset shape: (4932, 21)\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Run Preprocessing\n",
    "\n",
    "logging.info(f\"Attempting to load and preprocess data from: {param_file_path}\")\n",
    "\n",
    "processed_df = None \n",
    "\n",
    "try:\n",
    "    processed_df = load_and_preprocess_data(\n",
    "        file_path=param_file_path,\n",
    "        text_source_columns=param_text_source_columns,\n",
    "        unique_id_column=param_unique_id_column,\n",
    "        required_columns_for_docs_creation=param_required_columns_for_docs_creation, # New parameter\n",
    "        dropped_rows_output_path=param_dropped_rows_output_path,         # New parameter\n",
    "        data_type_specific_df_processing=param_data_type_specific_df_processing,\n",
    "        clean_apply_unescape=param_clean_apply_unescape,\n",
    "        clean_apply_url_removal=param_clean_apply_url_removal,\n",
    "        clean_apply_html_tag_removal=param_clean_apply_html_tag_removal,\n",
    "        clean_apply_quote_normalization=param_clean_apply_quote_normalization,\n",
    "        clean_apply_char_filtering=param_clean_apply_char_filtering,\n",
    "        clean_char_filter_regex=param_clean_char_filter_regex,\n",
    "        clean_apply_html_entity_removal=param_clean_apply_html_entity_removal,\n",
    "        clean_apply_lowercase=param_clean_apply_lowercase,\n",
    "        apply_length_filter=param_apply_length_filter,\n",
    "        min_doc_length=param_min_doc_length,\n",
    "        max_doc_length=param_max_doc_length,\n",
    "        apply_duplicate_removal=param_apply_duplicate_removal,\n",
    "        column_for_duplicate_checking=param_column_for_duplicate_checking,\n",
    "        apply_score_filter=param_apply_score_filter,\n",
    "        score_column_for_filtering=param_score_column_for_filtering,\n",
    "        min_score_for_filtering=param_min_score_for_filtering,\n",
    "        max_score_for_filtering=param_max_score_for_filtering\n",
    "    )\n",
    "\n",
    "    if processed_df is not None:\n",
    "        logging.info(f\"Preprocessing finished. Processed DataFrame shape: {processed_df.shape}\")\n",
    "        if not processed_df.empty:\n",
    "            try:\n",
    "                output_dir = os.path.dirname(processed_data_output_file)\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                    logging.info(f\"Created output directory: {output_dir}\")\n",
    "                processed_df.to_csv(processed_data_output_file, index=False)\n",
    "                logging.info(f\"Processed DataFrame saved to: {processed_data_output_file}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving processed DataFrame: {e}\")\n",
    "        else:\n",
    "            logging.info(\"Processed DataFrame is empty, not saving main output file.\")\n",
    "    else:\n",
    "        logging.warning(\"Preprocessing did not return a DataFrame.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Input file path error: {e}. Please ensure the path is correct.\")\n",
    "except ValueError as e:\n",
    "    logging.error(f\"Configuration or data error during preprocessing: {e}\")\n",
    "except NameError as e:\n",
    "     logging.error(f\"Import error - required function not loaded: {e}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred during preprocessing: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9940ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame Info (c:\\WORKING\\BERTopic_Modeling\\data\\processed\\s2_reddit_processed_docs.csv):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4932 entries, 0 to 5445\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   paperId                   4932 non-null   object\n",
      " 1   corpusId                  4932 non-null   int64 \n",
      " 2   url                       4932 non-null   object\n",
      " 3   title                     4932 non-null   object\n",
      " 4   abstract                  1413 non-null   object\n",
      " 5   venue                     4045 non-null   object\n",
      " 6   year                      4932 non-null   int64 \n",
      " 7   referenceCount            4932 non-null   int64 \n",
      " 8   citationCount             4932 non-null   int64 \n",
      " 9   influentialCitationCount  4932 non-null   int64 \n",
      " 10  isOpenAccess              4932 non-null   bool  \n",
      " 11  publicationDate           4193 non-null   object\n",
      " 12  author_names_str          4875 non-null   object\n",
      " 13  fieldsOfStudy_str         3648 non-null   object\n",
      " 14  s2FieldsOfStudy_str       4864 non-null   object\n",
      " 15  publicationTypes_str      3729 non-null   object\n",
      " 16  externalIds_json          4932 non-null   object\n",
      " 17  openAccessPdf_json        4932 non-null   object\n",
      " 18  journal_json              4572 non-null   object\n",
      " 19  publicationVenue_json     3297 non-null   object\n",
      " 20  docs                      4932 non-null   object\n",
      "dtypes: bool(1), int64(5), object(15)\n",
      "memory usage: 814.0+ KB\n",
      "\n",
      "First 5 rows of processed data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paperId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "corpusId",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "venue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "referenceCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citationCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "influentialCitationCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "isOpenAccess",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "publicationDate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "author_names_str",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fieldsOfStudy_str",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "s2FieldsOfStudy_str",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publicationTypes_str",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "externalIds_json",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "openAccessPdf_json",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "journal_json",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "publicationVenue_json",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "docs",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b665468d-efff-4249-8a0a-578c486133c9",
       "rows": [
        [
         "0",
         "0010ab98621bd8fa34de6b15e15b9da7fd28b4b3",
         "259553347",
         "https://www.semanticscholar.org/paper/0010ab98621bd8fa34de6b15e15b9da7fd28b4b3",
         "Reddit sentiment analysis for natural language processing",
         "In the Internet age, social media has fully penetrated into people's lives. As one of the well-developed online platforms with a large user base, Reddit allows users to independently publish current news, life experiences, and interesting life stories. However, sometimes it sends a negative tone that affects the brand of a company or individual and destroys profits and it is necessary to prevent Twitter by identifying hate words. The biggest innovation of this post is that we use reddit data to compare various methods simultaneously. As we process more data, trying deep learning will yield good results. Compared to other machine learning classifiers, the transformer classifier achieves the best results.",
         "Applied and Computational Engineering",
         "2023",
         "0",
         "0",
         "0",
         "True",
         "6/14/2023",
         "Ang Li",
         null,
         "Computer Science(s2-fos-model)",
         "JournalArticle",
         "{\"DOI\": \"10.54254/2755-2721/5/20230649\", \"CorpusId\": 259553347}",
         "{\"url\": \"https://www.ewadirect.com/proceedings/ace/article/view/2130/pdf\", \"status\": \"HYBRID\", \"license\": \"CCBY\", \"disclaimer\": \"Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/5/20230649?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/5/20230649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}",
         "{\"name\": \"Applied and Computational Engineering\"}",
         "{\"id\": \"38ef5a81-0fad-4de7-abc2-0fb847d3ece7\", \"name\": \"Applied and Computational Engineering\", \"type\": \"journal\", \"alternate_names\": [\"Appl Comput Eng\"], \"issn\": \"2755-2721\"}",
         "reddit sentiment analysis for natural language processing in the internet age, social media has fully penetrated into people's lives. as one of the well-developed online platforms with a large user base, reddit allows users to independently publish current news, life experiences, and interesting life stories. however, sometimes it sends a negative tone that affects the brand of a company or individual and destroys profits and it is necessary to prevent twitter by identifying hate words. the biggest innovation of this post is that we use reddit data to compare various methods simultaneously. as we process more data, trying deep learning will yield good results. compared to other machine learning classifiers, the transformer classifier achieves the best results."
        ],
        [
         "1",
         "001aebf4db7db55746eb9f4b53a8cc4e44ab64ff",
         "242991503",
         "https://www.semanticscholar.org/paper/001aebf4db7db55746eb9f4b53a8cc4e44ab64ff",
         "Author response for \"Reasoning in social media: insights from Reddit “Change My View” submissions\"",
         null,
         null,
         "2021",
         "0",
         "0",
         "0",
         "False",
         "2/23/2021",
         "Ayşe Öcal; Lu Xiao; Jaihyun Park",
         null,
         "Computer Science(s2-fos-model); Psychology(s2-fos-model)",
         null,
         "{\"DOI\": \"10.1108/oir-08-2020-0330/v3/response1\", \"CorpusId\": 242991503}",
         "{\"url\": \"\", \"status\": null, \"license\": null, \"disclaimer\": \"Notice: This paper's abstract has been elided by the publisher. Paper or abstract available at https://api.unpaywall.org/v2/10.1108/oir-08-2020-0330/v3/response1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/oir-08-2020-0330/v3/response1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}",
         null,
         null,
         "author response for \"reasoning in social media: insights from reddit \"change my view\" submissions\""
        ],
        [
         "2",
         "002559cc9c1b9168fe9fead1be9510eb1e5e80d4",
         "261572976",
         "https://www.semanticscholar.org/paper/002559cc9c1b9168fe9fead1be9510eb1e5e80d4",
         "Sharing Reliable COVID-19 Information and Countering Misinformation: In-Depth Interviews With Information Advocates",
         "Background The rampant spread of misinformation about COVID-19 has been linked to a lower uptake of preventive behaviors such as vaccination. Some individuals, however, have been able to resist believing in COVID-19 misinformation. Further, some have acted as information advocates, spreading accurate information and combating misinformation about the pandemic. Objective This work explores highly knowledgeable information advocates’ perspectives, behaviors, and information-related practices. Methods To identify participants for this study, we used outcomes of survey research of a national sample of 1498 adults to find individuals who scored a perfect or near-perfect score on COVID-19 knowledge questions and who also self-reported actively sharing or responding to news information within the past week. Among this subsample, we selected a diverse sample of 25 individuals to participate in a 1-time, phone-based, semistructured interview. Interviews were recorded and transcribed, and the team conducted an inductive thematic analysis. Results Participants reported trusting in science, data-driven sources, public health, medical experts, and organizations. They had mixed levels of trust in various social media sites to find reliable health information, noting distrust in particular sites such as Facebook (Meta Platforms) and more trust in specific accounts on Twitter (X Corp) and Reddit (Advance Publications). They reported relying on multiple sources of information to find facts instead of depending on their intuition and emotions to inform their perspectives about COVID-19. Participants determined the credibility of information by cross-referencing it, identifying information sources and their potential biases, clarifying information they were unclear about with health care providers, and using fact-checking sites to verify information. Most participants reported ignoring misinformation. Others, however, responded to misinformation by flagging, reporting, and responding to it on social media sites. Some described feeling more comfortable responding to misinformation in person than online. Participants’ responses to misinformation posted on the internet depended on various factors, including their relationship to the individual posting the misinformation, their level of outrage in response to it, and how dangerous they perceived it could be if others acted on such information. Conclusions This research illustrates how well-informed US adults assess the credibility of COVID-19 information, how they share it, and how they respond to misinformation. It illustrates web-based and offline information practices and describes how the role of interpersonal relationships contributes to their preferences for acting on such information. Implications of our findings could help inform future training in health information literacy, interpersonal information advocacy, and organizational information advocacy. It is critical to continue working to share reliable health information and debunk misinformation, particularly since this information informs health behaviors.",
         "JMIR infodemiology",
         "2023",
         "37",
         "2",
         "0",
         "True",
         "3/28/2023",
         "Alexis M. Koskan; Shalini Sivanandam; Kristy Roschke; Jonathan Irby; Deborah Helitzer; Bradley Doebbeling",
         "Medicine",
         "Medicine(external); Sociology(s2-fos-model)",
         "JournalArticle; Review",
         "{\"PubMedCentral\": \"10625073\", \"DOI\": \"10.2196/47677\", \"CorpusId\": 261572976, \"PubMed\": \"37862066\"}",
         "{\"url\": \"https://doi.org/10.2196/47677\", \"status\": \"GOLD\", \"license\": \"CCBY\", \"disclaimer\": \"Notice: This abstract is extracted from the open access paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10625073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}",
         "{\"name\": \"JMIR Infodemiology\", \"volume\": \"3\"}",
         "{\"id\": \"5954d9ea-100f-4b8f-94ce-3e2ba0431102\", \"name\": \"JMIR infodemiology\", \"type\": \"journal\", \"issn\": \"2564-1891\", \"url\": \"https://infodemiology.jmir.org/\"}",
         "sharing reliable covid-19 information and countering misinformation: in-depth interviews with information advocates background the rampant spread of misinformation about covid-19 has been linked to a lower uptake of preventive behaviors such as vaccination. some individuals, however, have been able to resist believing in covid-19 misinformation. further, some have acted as information advocates, spreading accurate information and combating misinformation about the pandemic. objective this work explores highly knowledgeable information advocates' perspectives, behaviors, and information-related practices. methods to identify participants for this study, we used outcomes of survey research of a national sample of 1498 adults to find individuals who scored a perfect or near-perfect score on covid-19 knowledge questions and who also self-reported actively sharing or responding to news information within the past week. among this subsample, we selected a diverse sample of 25 individuals to participate in a 1-time, phone-based, semistructured interview. interviews were recorded and transcribed, and the team conducted an inductive thematic analysis. results participants reported trusting in science, data-driven sources, public health, medical experts, and organizations. they had mixed levels of trust in various social media sites to find reliable health information, noting distrust in particular sites such as facebook meta platforms and more trust in specific accounts on twitter x corp and reddit advance publications. they reported relying on multiple sources of information to find facts instead of depending on their intuition and emotions to inform their perspectives about covid-19. participants determined the credibility of information by cross-referencing it, identifying information sources and their potential biases, clarifying information they were unclear about with health care providers, and using fact-checking sites to verify information. most participants reported ignoring misinformation. others, however, responded to misinformation by flagging, reporting, and responding to it on social media sites. some described feeling more comfortable responding to misinformation in person than online. participants' responses to misinformation posted on the internet depended on various factors, including their relationship to the individual posting the misinformation, their level of outrage in response to it, and how dangerous they perceived it could be if others acted on such information. conclusions this research illustrates how well-informed us adults assess the credibility of covid-19 information, how they share it, and how they respond to misinformation. it illustrates web-based and offline information practices and describes how the role of interpersonal relationships contributes to their preferences for acting on such information. implications of our findings could help inform future training in health information literacy, interpersonal information advocacy, and organizational information advocacy. it is critical to continue working to share reliable health information and debunk misinformation, particularly since this information informs health behaviors."
        ],
        [
         "4",
         "002788669c1548cedeeae6a0d6fd1e0d5eb40e43",
         "256758312",
         "https://www.semanticscholar.org/paper/002788669c1548cedeeae6a0d6fd1e0d5eb40e43",
         "Hate Speech Patterns in Social Media: A Methodological Framework and Fat Stigma Investigation Incorporating Sentiment Analysis, Topic Modelling and Discourse Analysis",
         "Social media offers users an online platform to freely express themselves; however, when users post opinionated and offensive comments that target certain individuals or communities, this could instigate animosity towards them. Widespread condemnation of obesity (fatness) has led to much fat stigmatizing content being posted online. A methodological framework that uses a novel mixed-method approach for unearthing hate speech patterns from large text-based corpora gathered from social media is proposed. We explain the use of computer-mediated quantitative methods comprising natural language processing techniques such as sentiment analysis, emotion analysis and topic modelling, along with qualitative discourse analysis. Next, we have applied the framework to a corpus of texts on gendered and weight-based data that have been extracted from Twitter and Reddit. This assisted in the detection of different emotions being expressed, the composition of word frequency patterns and the broader fat-based themes underpinning the hateful content posted online. The framework has provided a synthesis of quantitative and qualitative methods that draw on social science and data mining techniques to build real-world knowledge in hate speech detection. Current information systems research is limited in its use of mixed analytic approaches for studying hate speech in social media. Our study therefore contributes to future research by establishing a roadmap for conducting mixed-method analyses for better comprehension and understanding of hate speech patterns.",
         "Australasian Journal of Information Systems",
         "2023",
         "0",
         "4",
         "0",
         "True",
         "2/8/2023",
         "V. Wanniarachchi; C. Scogings; Teo Sušnjak; A. Mathrani",
         "Computer Science",
         "Computer Science(external); Sociology(s2-fos-model); Computer Science(s2-fos-model)",
         "JournalArticle",
         "{\"DBLP\": \"journals/ajis/WanniarachchiSSM23\", \"DOI\": \"10.3127/ajis.v27i0.3929\", \"CorpusId\": 256758312}",
         "{\"url\": \"https://journal.acs.org.au/index.php/ajis/article/download/3929/1307\", \"status\": \"GOLD\", \"license\": \"CCBYNC\", \"disclaimer\": \"Notice: This abstract is extracted from the open access paper or abstract available at https://api.unpaywall.org/v2/10.3127/ajis.v27i0.3929?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3127/ajis.v27i0.3929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}",
         "{\"name\": \"Australas. J. Inf. Syst.\", \"volume\": \"27\"}",
         "{\"id\": \"37046766-bc51-4464-99ed-f39d8391d1d8\", \"name\": \"Australasian Journal of Information Systems\", \"type\": \"journal\", \"alternate_names\": [\"Australas J Inf Syst\"], \"issn\": \"1039-7841\", \"url\": \"https://journal.acs.org.au/index.php/ajis\", \"alternate_urls\": [\"https://journal.acs.org.au/index.php/ajis/index\", \"https://web.archive.org/web/20050531003130/http:/www.uow.edu.au:80/ajis/ajis.html\", \"http://www.aaisnet.org/ajis/\"]}",
         "hate speech patterns in social media: a methodological framework and fat stigma investigation incorporating sentiment analysis, topic modelling and discourse analysis social media offers users an online platform to freely express themselves; however, when users post opinionated and offensive comments that target certain individuals or communities, this could instigate animosity towards them. widespread condemnation of obesity fatness has led to much fat stigmatizing content being posted online. a methodological framework that uses a novel mixed-method approach for unearthing hate speech patterns from large text-based corpora gathered from social media is proposed. we explain the use of computer-mediated quantitative methods comprising natural language processing techniques such as sentiment analysis, emotion analysis and topic modelling, along with qualitative discourse analysis. next, we have applied the framework to a corpus of texts on gendered and weight-based data that have been extracted from twitter and reddit. this assisted in the detection of different emotions being expressed, the composition of word frequency patterns and the broader fat-based themes underpinning the hateful content posted online. the framework has provided a synthesis of quantitative and qualitative methods that draw on social science and data mining techniques to build real-world knowledge in hate speech detection. current information systems research is limited in its use of mixed analytic approaches for studying hate speech in social media. our study therefore contributes to future research by establishing a roadmap for conducting mixed-method analyses for better comprehension and understanding of hate speech patterns."
        ],
        [
         "5",
         "0029c7cb16972c4c878bbef08ac0373363bf6d07",
         "265608098",
         "https://www.semanticscholar.org/paper/0029c7cb16972c4c878bbef08ac0373363bf6d07",
         "RoCS-MT: Robustness Challenge Set for Machine Translation",
         "RoCS-MT, a Robust Challenge Set for Machine Translation (MT), is designed to test MT systems’ ability to translate user-generated content (UGC) that displays non-standard characteristics, such as spelling errors, devowelling, acronymisation, etc. RoCS-MT is composed of English comments from Reddit, selected for their non-standard nature, which have been manually normalised and professionally translated into five languages: French, German, Czech, Ukrainian and Russian. In the context of the WMT23 test suite shared task, we analyse the models submitted to the general MT task for all from-English language pairs, offering some insights into the types of problems faced by state-of-the-art MT models when dealing with non-standard UGC texts. We compare automatic metrics for MT quality, including quality estimation to see if the same conclusions can be drawn without references. In terms of robustness, we find that many of the systems struggle with non-standard variants of words (e.g. due to phonetically inspired spellings, contraction, truncations, etc.), but that this depends on the system and the amount of training data, with the best overall systems performing better across all phenomena. GPT4 is the clear front-runner. However we caution against drawing conclusions about generalisation capacity as it and other systems could be trained on the source side of RoCS and also on similar data.",
         "Conference on Machine Translation",
         "2023",
         "35",
         "6",
         "0",
         "True",
         null,
         "Rachel Bawden; Benoît Sagot",
         "Computer Science",
         "Computer Science(external); Computer Science(s2-fos-model); Linguistics(s2-fos-model)",
         "JournalArticle",
         "{\"ACL\": \"2023.wmt-1.21\", \"DBLP\": \"conf/wmt/BawdenS23\", \"DOI\": \"10.18653/v1/2023.wmt-1.21\", \"CorpusId\": 265608098}",
         "{\"url\": \"https://aclanthology.org/2023.wmt-1.21.pdf\", \"status\": \"HYBRID\", \"license\": \"CCBY\", \"disclaimer\": \"Notice: This abstract is extracted from the open access paper or abstract available at https://aclanthology.org/2023.wmt-1.21, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}",
         "{\"pages\": \"198-216\"}",
         "{\"id\": \"9aacb914-3edf-4e02-b8fe-5abf21c4d2ba\", \"name\": \"Conference on Machine Translation\", \"type\": \"conference\", \"alternate_names\": [\"WMT\", \"Conf Mach Transl\"]}",
         "rocs-mt: robustness challenge set for machine translation rocs-mt, a robust challenge set for machine translation mt, is designed to test mt systems' ability to translate user-generated content ugc that displays non-standard characteristics, such as spelling errors, devowelling, acronymisation, etc. rocs-mt is composed of english comments from reddit, selected for their non-standard nature, which have been manually normalised and professionally translated into five languages: french, german, czech, ukrainian and russian. in the context of the wmt23 test suite shared task, we analyse the models submitted to the general mt task for all from-english language pairs, offering some insights into the types of problems faced by state-of-the-art mt models when dealing with non-standard ugc texts. we compare automatic metrics for mt quality, including quality estimation to see if the same conclusions can be drawn without references. in terms of robustness, we find that many of the systems struggle with non-standard variants of words e.g. due to phonetically inspired spellings, contraction, truncations, etc., but that this depends on the system and the amount of training data, with the best overall systems performing better across all phenomena. gpt4 is the clear front-runner. however we caution against drawing conclusions about generalisation capacity as it and other systems could be trained on the source side of rocs and also on similar data."
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>corpusId</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>referenceCount</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "      <th>...</th>\n",
       "      <th>publicationDate</th>\n",
       "      <th>author_names_str</th>\n",
       "      <th>fieldsOfStudy_str</th>\n",
       "      <th>s2FieldsOfStudy_str</th>\n",
       "      <th>publicationTypes_str</th>\n",
       "      <th>externalIds_json</th>\n",
       "      <th>openAccessPdf_json</th>\n",
       "      <th>journal_json</th>\n",
       "      <th>publicationVenue_json</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0010ab98621bd8fa34de6b15e15b9da7fd28b4b3</td>\n",
       "      <td>259553347</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0010ab98...</td>\n",
       "      <td>Reddit sentiment analysis for natural language...</td>\n",
       "      <td>In the Internet age, social media has fully pe...</td>\n",
       "      <td>Applied and Computational Engineering</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6/14/2023</td>\n",
       "      <td>Ang Li</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computer Science(s2-fos-model)</td>\n",
       "      <td>JournalArticle</td>\n",
       "      <td>{\"DOI\": \"10.54254/2755-2721/5/20230649\", \"Corp...</td>\n",
       "      <td>{\"url\": \"https://www.ewadirect.com/proceedings...</td>\n",
       "      <td>{\"name\": \"Applied and Computational Engineering\"}</td>\n",
       "      <td>{\"id\": \"38ef5a81-0fad-4de7-abc2-0fb847d3ece7\",...</td>\n",
       "      <td>reddit sentiment analysis for natural language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001aebf4db7db55746eb9f4b53a8cc4e44ab64ff</td>\n",
       "      <td>242991503</td>\n",
       "      <td>https://www.semanticscholar.org/paper/001aebf4...</td>\n",
       "      <td>Author response for \"Reasoning in social media...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2/23/2021</td>\n",
       "      <td>Ayşe Öcal; Lu Xiao; Jaihyun Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computer Science(s2-fos-model); Psychology(s2-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"DOI\": \"10.1108/oir-08-2020-0330/v3/response1...</td>\n",
       "      <td>{\"url\": \"\", \"status\": null, \"license\": null, \"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>author response for \"reasoning in social media...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002559cc9c1b9168fe9fead1be9510eb1e5e80d4</td>\n",
       "      <td>261572976</td>\n",
       "      <td>https://www.semanticscholar.org/paper/002559cc...</td>\n",
       "      <td>Sharing Reliable COVID-19 Information and Coun...</td>\n",
       "      <td>Background The rampant spread of misinformatio...</td>\n",
       "      <td>JMIR infodemiology</td>\n",
       "      <td>2023</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3/28/2023</td>\n",
       "      <td>Alexis M. Koskan; Shalini Sivanandam; Kristy R...</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Medicine(external); Sociology(s2-fos-model)</td>\n",
       "      <td>JournalArticle; Review</td>\n",
       "      <td>{\"PubMedCentral\": \"10625073\", \"DOI\": \"10.2196/...</td>\n",
       "      <td>{\"url\": \"https://doi.org/10.2196/47677\", \"stat...</td>\n",
       "      <td>{\"name\": \"JMIR Infodemiology\", \"volume\": \"3\"}</td>\n",
       "      <td>{\"id\": \"5954d9ea-100f-4b8f-94ce-3e2ba0431102\",...</td>\n",
       "      <td>sharing reliable covid-19 information and coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002788669c1548cedeeae6a0d6fd1e0d5eb40e43</td>\n",
       "      <td>256758312</td>\n",
       "      <td>https://www.semanticscholar.org/paper/00278866...</td>\n",
       "      <td>Hate Speech Patterns in Social Media: A Method...</td>\n",
       "      <td>Social media offers users an online platform t...</td>\n",
       "      <td>Australasian Journal of Information Systems</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2/8/2023</td>\n",
       "      <td>V. Wanniarachchi; C. Scogings; Teo Sušnjak; A....</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Science(external); Sociology(s2-fos-m...</td>\n",
       "      <td>JournalArticle</td>\n",
       "      <td>{\"DBLP\": \"journals/ajis/WanniarachchiSSM23\", \"...</td>\n",
       "      <td>{\"url\": \"https://journal.acs.org.au/index.php/...</td>\n",
       "      <td>{\"name\": \"Australas. J. Inf. Syst.\", \"volume\":...</td>\n",
       "      <td>{\"id\": \"37046766-bc51-4464-99ed-f39d8391d1d8\",...</td>\n",
       "      <td>hate speech patterns in social media: a method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0029c7cb16972c4c878bbef08ac0373363bf6d07</td>\n",
       "      <td>265608098</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0029c7cb...</td>\n",
       "      <td>RoCS-MT: Robustness Challenge Set for Machine ...</td>\n",
       "      <td>RoCS-MT, a Robust Challenge Set for Machine Tr...</td>\n",
       "      <td>Conference on Machine Translation</td>\n",
       "      <td>2023</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rachel Bawden; Benoît Sagot</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Computer Science(external); Computer Science(s...</td>\n",
       "      <td>JournalArticle</td>\n",
       "      <td>{\"ACL\": \"2023.wmt-1.21\", \"DBLP\": \"conf/wmt/Baw...</td>\n",
       "      <td>{\"url\": \"https://aclanthology.org/2023.wmt-1.2...</td>\n",
       "      <td>{\"pages\": \"198-216\"}</td>\n",
       "      <td>{\"id\": \"9aacb914-3edf-4e02-b8fe-5abf21c4d2ba\",...</td>\n",
       "      <td>rocs-mt: robustness challenge set for machine ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId   corpusId  \\\n",
       "0  0010ab98621bd8fa34de6b15e15b9da7fd28b4b3  259553347   \n",
       "1  001aebf4db7db55746eb9f4b53a8cc4e44ab64ff  242991503   \n",
       "2  002559cc9c1b9168fe9fead1be9510eb1e5e80d4  261572976   \n",
       "4  002788669c1548cedeeae6a0d6fd1e0d5eb40e43  256758312   \n",
       "5  0029c7cb16972c4c878bbef08ac0373363bf6d07  265608098   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.semanticscholar.org/paper/0010ab98...   \n",
       "1  https://www.semanticscholar.org/paper/001aebf4...   \n",
       "2  https://www.semanticscholar.org/paper/002559cc...   \n",
       "4  https://www.semanticscholar.org/paper/00278866...   \n",
       "5  https://www.semanticscholar.org/paper/0029c7cb...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Reddit sentiment analysis for natural language...   \n",
       "1  Author response for \"Reasoning in social media...   \n",
       "2  Sharing Reliable COVID-19 Information and Coun...   \n",
       "4  Hate Speech Patterns in Social Media: A Method...   \n",
       "5  RoCS-MT: Robustness Challenge Set for Machine ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In the Internet age, social media has fully pe...   \n",
       "1                                                NaN   \n",
       "2  Background The rampant spread of misinformatio...   \n",
       "4  Social media offers users an online platform t...   \n",
       "5  RoCS-MT, a Robust Challenge Set for Machine Tr...   \n",
       "\n",
       "                                         venue  year  referenceCount  \\\n",
       "0        Applied and Computational Engineering  2023               0   \n",
       "1                                          NaN  2021               0   \n",
       "2                           JMIR infodemiology  2023              37   \n",
       "4  Australasian Journal of Information Systems  2023               0   \n",
       "5            Conference on Machine Translation  2023              35   \n",
       "\n",
       "   citationCount  influentialCitationCount  ...  publicationDate  \\\n",
       "0              0                         0  ...        6/14/2023   \n",
       "1              0                         0  ...        2/23/2021   \n",
       "2              2                         0  ...        3/28/2023   \n",
       "4              4                         0  ...         2/8/2023   \n",
       "5              6                         0  ...              NaN   \n",
       "\n",
       "                                    author_names_str fieldsOfStudy_str  \\\n",
       "0                                             Ang Li               NaN   \n",
       "1                   Ayşe Öcal; Lu Xiao; Jaihyun Park               NaN   \n",
       "2  Alexis M. Koskan; Shalini Sivanandam; Kristy R...          Medicine   \n",
       "4  V. Wanniarachchi; C. Scogings; Teo Sušnjak; A....  Computer Science   \n",
       "5                        Rachel Bawden; Benoît Sagot  Computer Science   \n",
       "\n",
       "                                 s2FieldsOfStudy_str    publicationTypes_str  \\\n",
       "0                     Computer Science(s2-fos-model)          JournalArticle   \n",
       "1  Computer Science(s2-fos-model); Psychology(s2-...                     NaN   \n",
       "2        Medicine(external); Sociology(s2-fos-model)  JournalArticle; Review   \n",
       "4  Computer Science(external); Sociology(s2-fos-m...          JournalArticle   \n",
       "5  Computer Science(external); Computer Science(s...          JournalArticle   \n",
       "\n",
       "                                    externalIds_json  \\\n",
       "0  {\"DOI\": \"10.54254/2755-2721/5/20230649\", \"Corp...   \n",
       "1  {\"DOI\": \"10.1108/oir-08-2020-0330/v3/response1...   \n",
       "2  {\"PubMedCentral\": \"10625073\", \"DOI\": \"10.2196/...   \n",
       "4  {\"DBLP\": \"journals/ajis/WanniarachchiSSM23\", \"...   \n",
       "5  {\"ACL\": \"2023.wmt-1.21\", \"DBLP\": \"conf/wmt/Baw...   \n",
       "\n",
       "                                  openAccessPdf_json  \\\n",
       "0  {\"url\": \"https://www.ewadirect.com/proceedings...   \n",
       "1  {\"url\": \"\", \"status\": null, \"license\": null, \"...   \n",
       "2  {\"url\": \"https://doi.org/10.2196/47677\", \"stat...   \n",
       "4  {\"url\": \"https://journal.acs.org.au/index.php/...   \n",
       "5  {\"url\": \"https://aclanthology.org/2023.wmt-1.2...   \n",
       "\n",
       "                                        journal_json  \\\n",
       "0  {\"name\": \"Applied and Computational Engineering\"}   \n",
       "1                                                NaN   \n",
       "2      {\"name\": \"JMIR Infodemiology\", \"volume\": \"3\"}   \n",
       "4  {\"name\": \"Australas. J. Inf. Syst.\", \"volume\":...   \n",
       "5                               {\"pages\": \"198-216\"}   \n",
       "\n",
       "                               publicationVenue_json  \\\n",
       "0  {\"id\": \"38ef5a81-0fad-4de7-abc2-0fb847d3ece7\",...   \n",
       "1                                                NaN   \n",
       "2  {\"id\": \"5954d9ea-100f-4b8f-94ce-3e2ba0431102\",...   \n",
       "4  {\"id\": \"37046766-bc51-4464-99ed-f39d8391d1d8\",...   \n",
       "5  {\"id\": \"9aacb914-3edf-4e02-b8fe-5abf21c4d2ba\",...   \n",
       "\n",
       "                                                docs  \n",
       "0  reddit sentiment analysis for natural language...  \n",
       "1  author response for \"reasoning in social media...  \n",
       "2  sharing reliable covid-19 information and coun...  \n",
       "4  hate speech patterns in social media: a method...  \n",
       "5  rocs-mt: robustness challenge set for machine ...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of 'docs' column (first 3 documents):\n",
      "Doc 1: reddit sentiment analysis for natural language processing in the internet age, social media has fully penetrated into people's lives. as one of the well-developed online platforms with a large user ba...\n",
      "Doc 2: author response for \"reasoning in social media: insights from reddit \"change my view\" submissions\"...\n",
      "Doc 3: sharing reliable covid-19 information and countering misinformation: in-depth interviews with information advocates background the rampant spread of misinformation about covid-19 has been linked to a ...\n",
      "\n",
      "Output file should be at: c:\\WORKING\\BERTopic_Modeling\\data\\processed\\s2_reddit_processed_docs.csv\n",
      "Does output file exist? True\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Inspect Output\n",
    "\n",
    "from IPython.display import display \n",
    "\n",
    "print(\"--- Main Processed DataFrame --- \")\n",
    "if processed_df is not None and not processed_df.empty:\n",
    "    print(f\"\\nProcessed DataFrame Info ({processed_data_output_file}):\")\n",
    "    processed_df.info()\n",
    "    print(\"\\nFirst 5 rows of processed data:\")\n",
    "    display(processed_df.head())\n",
    "    if 'docs' in processed_df.columns:\n",
    "        print(\"\\nSample of 'docs' column (first 3 documents):\")\n",
    "        for i, doc in enumerate(processed_df['docs'].head(3)):\n",
    "            print(f\"Doc {i+1}: {doc[:200]}...\") \n",
    "    print(f\"\\nOutput file should be at: {processed_data_output_file}\")\n",
    "    print(f\"Does output file exist? {os.path.exists(processed_data_output_file)}\")\n",
    "elif processed_df is not None and processed_df.empty:\n",
    "     print(\"\\nPreprocessing resulted in an empty DataFrame. Check filters and source data.\")\n",
    "     print(f\"Output file path specified: {processed_data_output_file}\")\n",
    "     print(f\"Does (potentially empty) output file exist? {os.path.exists(processed_data_output_file)}\")\n",
    "else:\n",
    "    print(\"\\nPreprocessing failed or DataFrame was not created/returned correctly.\")\n",
    "    print(f\"Expected output file: {processed_data_output_file}\")\n",
    "\n",
    "print(\"\\n--- Dropped Rows (due to missing required columns) --- \")\n",
    "if os.path.exists(dropped_rows_file):\n",
    "    try:\n",
    "        df_dropped_check = pd.read_csv(dropped_rows_file)\n",
    "        print(f\"Successfully loaded dropped rows file: {dropped_rows_file}\")\n",
    "        print(f\"Number of rows dropped due to missing required columns: {len(df_dropped_check)}\")\n",
    "        if not df_dropped_check.empty:\n",
    "            print(\"\\nFirst 5 rows of dropped data:\")\n",
    "            display(df_dropped_check.head())\n",
    "        else:\n",
    "            print(\"The dropped rows file is empty (meaning no rows met the criteria for being dropped due to missing required columns initially, or they were filtered out by other means).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or inspecting dropped rows file {dropped_rows_file}: {e}\")\n",
    "else:\n",
    "    print(f\"Dropped rows file not found at: {dropped_rows_file}. This is expected if no rows were dropped for missing required columns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
