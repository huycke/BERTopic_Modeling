{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths\n",
    "data_path = 'rpg/rpg_small_processed.csv'\n",
    "embeddings_save_path = 'rpg/rpg_small_processed_embeddings.npy'\n",
    "\n",
    "print(\"Step 1: Loading the data...\")\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Create embeddings\n",
    "model = SentenceTransformer('thenlper/gte-small')\n",
    "embeddings = model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Save Embeddings\n",
    "with open(embeddings_save_path, 'wb') as f:\n",
    "    np.save(f, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import collections\n",
    "from bertopic.representation import PartOfSpeech\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from tqdm import tqdm\n",
    "from bertopic.cluster import BaseCluster\n",
    "\n",
    "# Define file paths\n",
    "data_path = 'attachment/attach_processed_length10.csv'\n",
    "embeddings_save_path = 'attachment/doc/models/attach_doc_embeddings.npy'\n",
    "model_save_path = \"attachment/doc/models/attach_doc6_model.pkl\"\n",
    "data_save_path = \"attachment/doc/models/attach_doc6_data.pkl\"\n",
    "\n",
    "print(\"Step 1: Loading the data...\")\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Load Embeddings\n",
    "with open(embeddings_save_path, 'rb') as f:\n",
    "    embeddings = np.load(f)\n",
    "\n",
    "###### Extract vocab to be used in BERTopic\n",
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer(ngram_range=(1, 3)).build_tokenizer()\n",
    "for doc in tqdm(docs):\n",
    "    vocab.update(tokenizer(doc))\n",
    "vocab = [word for word, frequency in vocab.items() if frequency >= 200]; len(vocab)\n",
    "\n",
    "# Train model and reduce dimensionality of embeddings\n",
    "\n",
    "umap_model = UMAP(\n",
    "        n_components=5,  # has a wild impact hard to predict\n",
    "        n_neighbors=30,  # Higher is a more gloabl strcture\n",
    "        min_dist=0.0,   # Lower value means more dense packing\n",
    "        random_state=42, # Reproducability\n",
    "        metric=\"cosine\", # have to pick something\n",
    "        n_jobs=-1        # speed\n",
    "        )\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# Find clusters of semantically similar documents\n",
    "hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=100,           # smallest size group considered\n",
    "            min_samples=50,                 # larger is more conservative - more noise\n",
    "            leaf_size=40,                   # number of points per leaf node in the tree - default 40\n",
    "            gen_min_span_tree=False,        # True creates minimum spanning trees - increasing RAM\n",
    "            prediction_data=True,           # generates extra cached data of prediction labels for new data or reuse\n",
    "            cluster_selection_method='leaf', # eom is normal - leaf might get more homogeneous clusters\n",
    "            cluster_selection_epsilon=0.0,  # default - merges clusters below threshold\n",
    "            core_dist_n_jobs=-1,            # For speed\n",
    "            )\n",
    "clusters = hdbscan_model.fit(reduced_embeddings).labels_\n",
    "\n",
    "class Dimensionality:\n",
    "    \"\"\" Use this for pre-calculated reduced embeddings \"\"\"\n",
    "    def __init__(self, reduced_embeddings):\n",
    "        self.reduced_embeddings = reduced_embeddings\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.reduced_embeddings\n",
    "\n",
    "# Set the main_representation for the model\n",
    "main_representation = KeyBERTInspired()\n",
    "\n",
    "# Additional ways of representing a topic\n",
    "aspect_model = [KeyBERTInspired(top_n_words=10), MaximalMarginalRelevance(diversity=.3)]\n",
    "\n",
    "# Prepare sub-models\n",
    "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
    "umap_model = Dimensionality(reduced_embeddings)\n",
    "hdbscan_model = BaseCluster()\n",
    "vectorizer_model = CountVectorizer(vocabulary=vocab, stop_words=\"english\")\n",
    "representation_model = {\n",
    "    \"Main\": main_representation,\n",
    "    \"Aspect1\": aspect_model,\n",
    "}\n",
    "\n",
    "# Fit BERTopic without actually performing any clustering\n",
    "topic_model= BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit model and transform documents\n",
    "topics, _= topic_model.fit_transform(docs, embeddings=embeddings, y=clusters)\n",
    "\n",
    "# Load the full dataset\n",
    "full_data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Add the topics to the full dataset\n",
    "# Ensure the length of `topics` matches the number of rows in `full_data`\n",
    "full_data['topics'] = topics\n",
    "\n",
    "# Save the BERTopic model and the full dataset with topics to .pkl files\n",
    "with open(model_save_path, \"wb\") as model_file:\n",
    "    pickle.dump(topic_model, model_file)\n",
    "\n",
    "with open(data_save_path, \"wb\") as data_file:\n",
    "    pickle.dump(full_data, data_file)\n",
    "\n",
    "print(\"Data and model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Extract the results\n",
    "topics = topic_model.get_topics()\n",
    "topic_freq = topic_model.get_topic_freq()\n",
    "topic_info = topic_model.get_topic_info()\n",
    "representative_docs = topic_model.get_representative_docs()\n",
    "\n",
    "# Assuming 'topic_info' is already defined and includes topic representations\n",
    "csv_file_path = 'rpg/analysis/rpg_small_processed2_analysis.csv'  # Hardcoded save location for CSV\n",
    "\n",
    "# Remove the 'Representative_Docs' column from topic_info DataFrame\n",
    "topic_info = topic_info.drop(columns=['Representative_Docs'])\n",
    "\n",
    "# Save the results in a more structured and readable manner\n",
    "with open('rpg/analysis/rpg_small_processed2_analysis.txt', 'w') as f:\n",
    "    # Topics\n",
    "    f.write(\"TOPICS:\\n\")\n",
    "    for topic_num, terms in topics.items():\n",
    "        terms_str = ', '.join([term[0] for term in terms])\n",
    "        f.write(f\"Topic {topic_num}: {terms_str}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # Topic Frequency\n",
    "    f.write(\"TOPIC FREQUENCY:\\n\")\n",
    "    for index, row in topic_freq.iterrows():\n",
    "        f.write(f\"Topic {row['Topic']}: {row['Count']} entries\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # Topic Info\n",
    "    f.write(\"TOPIC INFO:\\n\")\n",
    "    for index, row in topic_info.iterrows():\n",
    "        f.write(f\"Topic {row['Topic']}\\n\")\n",
    "        f.write(f\" - Name: {row['Name']}\\n\")\n",
    "        f.write(\" - Representation:\\n\")\n",
    "        for term in row['Representation']:\n",
    "            f.write(f\"   * {term}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    # Representative Docs\n",
    "    f.write(\"REPRESENTATIVE DOCS:\\n\")\n",
    "    for topic_num, docs in representative_docs.items():\n",
    "        f.write(f\"Topic {topic_num} representative docs:\\n\")\n",
    "        for doc in docs:\n",
    "            f.write(f\" - {doc}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Convert 'topic_info' DataFrame directly to CSV\n",
    "topic_info.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the original embeddings\n",
    "topic_model.visualize_document_datamap(docs, embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def explore_relevant_topics_to_file(topic_model, search_terms, txt_filename, csv_filename, top_n=5):\n",
    "    \"\"\"\n",
    "    Find and save topics related to a list of search terms to a .txt file and a .csv file,\n",
    "    along with representative documents for the topics.\n",
    "\n",
    "    Parameters:\n",
    "    - topic_model: The trained BERTopic model.\n",
    "    - search_terms: A list of search terms/phrases related to the desired topics.\n",
    "    - txt_filename: Name of the .txt file to save the results.\n",
    "    - csv_filename: Name of the .csv file to save the topic information.\n",
    "    - top_n: Number of top similar topics to retrieve for each search term.\n",
    "\n",
    "    Returns:\n",
    "    - None (writes the relevant topics, their terms, and representative docs to a .txt file\n",
    "             and topic information to a .csv file)\n",
    "    \"\"\"\n",
    "    topics_covered = set()  # To keep track of topics we've added representative docs for\n",
    "    all_relevant_topics = set()  # To gather all unique topics from the search results\n",
    "    topic_info_data = []  # To store topic information for the CSV file\n",
    "\n",
    "    with open(txt_filename, 'w') as file:\n",
    "        # Display search terms and their related topics at the top\n",
    "        for term in search_terms:\n",
    "            file.write(f\"Searching for topics related to: '{term}'\\n\\n\")\n",
    "            topics, similarity = topic_model.find_topics(term, top_n=top_n)\n",
    "            for topic, score in zip(topics, similarity):\n",
    "                file.write(f\"Topic {topic} (Similarity: {score:.4f})\\n\")\n",
    "                all_relevant_topics.add(topic)  # Add topic to the set\n",
    "                topic_info_data.append({'Topic': topic, 'Representation': tuple(topic_model.get_topic(topic)), 'Search Term': term})\n",
    "            file.write(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "        # Append topic details and representative documents at the end in numerical order\n",
    "        for topic in sorted(all_relevant_topics):  # Sort topics numerically\n",
    "            if topic not in topics_covered:\n",
    "                topic_terms = topic_model.get_topic(topic)\n",
    "                formatted_terms = ', '.join([f\"{word[0]} ({word[1]:.4f})\" for word in topic_terms])\n",
    "                file.write(f\"\\nTopic {topic} Details: {formatted_terms}\\n\\n\")\n",
    "                reps = topic_model.get_representative_docs(topic)\n",
    "                file.write(f\"Representative Documents for Topic {topic}:\\n\")\n",
    "                for doc in reps:\n",
    "                    file.write(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "                    file.write(f\"{doc}\\n\")\n",
    "                    file.write(\"-\" * 30 + \"\\n\")\n",
    "                topics_covered.add(topic)\n",
    "            file.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # Create a DataFrame from the topic information data\n",
    "    topic_info_df = pd.DataFrame(topic_info_data)\n",
    "    \n",
    "    # Aggregate search terms for each topic\n",
    "    topic_info_df = topic_info_df.groupby(['Topic', 'Representation'])['Search Term'].apply(', '.join).reset_index()\n",
    "    \n",
    "    # Save the topic information to a CSV file\n",
    "    topic_info_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Updated list of search terms related to your research question\n",
    "search_terms = [\n",
    "    \"Boundaries\", \"Limits\", \"Personal space\", \"Assertiveness\", \"Saying no\",\n",
    "    \"Interpersonal boundaries\", \"Relationship limits\", \"Healthy relationships\", \"Assertive communication\",\n",
    "    \"Personal growth\", \"Self-improvement\", \"Interpersonal skills\", \"Relationship building\",\n",
    "    \"Communication skills\", \"Active listening\", \"Expressing emotions\", \"Nonverbal communication\",\n",
    "    \"Self-care\", \"Self-compassion\", \"Mental health\", \"Emotional well-being\", \"Self-love\",\n",
    "    \"Social connection\", \"Belonging\", \"Interpersonal relationships\", \"Social support\", \"Emotional intimacy\",\n",
    "    \"Roleplaying\", \"Immersion\", \"Character development\", \"Alternate persona\", \"Escapism\",\n",
    "    \"Authenticity\", \"Self-expression\", \"Identity exploration\", \"True self\",\n",
    "    \"Anxiety relief\", \"Depression relief\", \"Therapeutic gaming\", \"Relaxation\",\n",
    "    \"Player growth\", \"Personal development\", \"Supportive environment\", \"Encouraging rules\",\n",
    "    \"Player education\", \"Mentoring\", \"Skill development\", \"Collaborative learning\",\n",
    "    \"Flexibility\", \"Adaptability\", \"Open-mindedness\", \"Embracing change\",\n",
    "    \"Time management\", \"Session planning\", \"Consistency\", \"Commitment\",\n",
    "    \"Responsibility\", \"Maturity\", \"Life skills\", \"Independence\",\n",
    "    \"Emotional intelligence\", \"Emotional regulation\", \"Self-awareness\", \"Empathy\",\n",
    "    \"Safety\", \"Security\", \"Trust\", \"Comfort\", \"Supportive environment\",\n",
    "    \"Trauma recovery\", \"Emotional healing\", \"Therapeutic roleplaying\", \"Coping mechanisms\",\n",
    "    \"Genuineness\", \"Honesty\",\n",
    "    \"Recognition\", \"Acknowledgment\", \"Validation\", \"Acceptance\",\n",
    "    \"Inclusivity\", \"Self-acceptance\",\n",
    "    \"Affirmation\", \"Support\", \"Encouragement\", \"Understanding\",\n",
    "    \"Resilience\", \"Perseverance\", \"Problem-solving\", \"Determination\", \"Growth mindset\"\n",
    "]\n",
    "\n",
    "# Use the function to explore the relevant topics and save to a .txt file and a .csv file\n",
    "txt_filename = \"rpg/analysis/rpg_small_processed2_relevant_topics.txt\"\n",
    "csv_filename = \"rpg/analysis/rpg_small_processed2_relevant_topics.csv\"\n",
    "explore_relevant_topics_to_file(topic_model, search_terms, txt_filename, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to properly load the documents - causing an error now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already loaded the BERTopic model and have the docs list\n",
    "# Step 1: Get the document info\n",
    "document_info = topic_model.get_document_info(docs)\n",
    "\n",
    "# List of topics you want to extract\n",
    "topics_to_include = [82, 332]\n",
    "\n",
    "# Step 2: Filter the DataFrame by the given set of topics\n",
    "filtered_df = document_info[document_info['Topic'].isin(topics_to_include)]\n",
    "\n",
    "# Step 3: Select only the relevant columns\n",
    "selected_df = filtered_df[['Document', 'Topic', 'Probability', 'Representation']]\n",
    "\n",
    "# Step 4: Save the selected DataFrame to a .csv file with all documents\n",
    "selected_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "# Step 5: Create a DataFrame with only 200 documents for each topic\n",
    "limited_df = pd.concat([filtered_df[filtered_df['Topic'] == topic].sample(min(len(filtered_df[filtered_df['Topic'] == topic]), 200)) \n",
    "                        for topic in topics_to_include])\n",
    "\n",
    "# Step 6: Save the limited DataFrame to a .csv file with only 200 documents per topic\n",
    "limited_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, similarity = topic_model.find_topics(\"Healing through my trauma.\", top_n=5)\n",
    "for topic, score in zip(topics, similarity):\n",
    "    print(f\"Topic {topic} (Similarity: {score:.4f}): {topic_model.get_topic(topic)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, similarity = topic_model.find_topics(\"Finding out my real identity, sense of self, and who I am.\", top_n=5)\n",
    "for topic, score in zip(topics, similarity):\n",
    "    print(f\"Topic {topic} (Similarity: {score:.4f}): {topic_model.get_topic(topic)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, similarity = topic_model.find_topics(\"Learning how to set boundaries and love myself.\", top_n=5)\n",
    "for topic, score in zip(topics, similarity):\n",
    "    print(f\"Topic {topic} (Similarity: {score:.4f}): {topic_model.get_topic(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, similarity = topic_model.find_topics(\"No DnD is better than bad DnD\", top_n=5)\n",
    "for topic, score in zip(topics, similarity):\n",
    "    print(f\"Topic {topic} (Similarity: {score:.4f}): {topic_model.get_topic(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, similarity = topic_model.find_topics(\"How can I learn to talk with problematic players?\", top_n=5)\n",
    "for topic, score in zip(topics, similarity):\n",
    "    print(f\"Topic {topic} (Similarity: {score:.4f}): {topic_model.get_topic(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, similarity = topic_model.find_topics(\"Red flags and problem players\", top_n=5)\n",
    "for topic, score in zip(topics, similarity):\n",
    "    print(f\"Topic {topic} (Similarity: {score:.4f}): {topic_model.get_topic(topic)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
