{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to combine and clean and filter folder of csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Make the *_combined.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "input_directory = 'rpg/csv_working/'\n",
    "consolidated_csv_path = 'rpg/rpg_small.csv'\n",
    "\n",
    "filtered_subreddits = [\n",
    "    \"AskGameMasters\",\n",
    "    \"DMAcademy\",\n",
    "    \"dndhorrorstories\",\n",
    "    \"DungeonMasters\",\n",
    "    \"lfg\",\n",
    "    \"rpg\",\n",
    "    \"rpghorrorstories\"\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\") and any(subreddit.lower() in filename.lower() for subreddit in filtered_subreddits):\n",
    "        filepath = os.path.join(input_directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Determine the source type and create 'text' column accordingly\n",
    "        if '_comments' in filename:\n",
    "            # For comments, use 'body' as the text\n",
    "            df['text'] = df['body'].fillna('')\n",
    "        else:\n",
    "            # For submissions, concatenate 'title' and 'selftext'\n",
    "            df['title'] = df['title'].fillna('')\n",
    "            df['selftext'] = df['selftext'].fillna('')\n",
    "            df['text'] = df['title'] + ' ' + df['selftext']\n",
    "        \n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame with the 'text' column to a new CSV file\n",
    "combined_df.to_csv(consolidated_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined DataFrame with 'text' column saved to {consolidated_csv_path}. Total rows: {len(combined_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess the *combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from html import unescape\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "consolidated_csv_path = 'rpg/rpg_small.csv'\n",
    "preprocessed_csv_path = 'rpg/rpg_small_preprocessed.csv'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced text data preprocessing, returning an empty string for unsuitable content.\"\"\"\n",
    "    if pd.isna(text) or re.match(r'^\\s*$', str(text)):\n",
    "        return ''\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"â€™|‘|’\", \"'\", text)\n",
    "    text = re.sub(r'“|”', '\"', text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s,.!?':;\\\"-]\", \"\", text)\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    text = re.sub(r'&[a-zA-Z]+;', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "# Load the combined DataFrame directly from the CSV\n",
    "combined_df = pd.read_csv(consolidated_csv_path, dtype=str, low_memory=False)\n",
    "\n",
    "# Remove rows with '[deleted]' or '[removed]' before any other preprocessing\n",
    "patterns_to_remove = r'\\[deleted\\]|\\[removed\\]|\\[deleted by user\\]|\\[removed by user\\]'\n",
    "for column in ['body', 'title', 'selftext']:\n",
    "    if column in combined_df.columns:\n",
    "        combined_df = combined_df[~combined_df[column].str.contains(patterns_to_remove, na=False, regex=True)]\n",
    "\n",
    "# Apply preprocessing to text columns\n",
    "for column in ['body', 'title', 'selftext']:\n",
    "    if column in combined_df.columns:\n",
    "        combined_df[column] = combined_df[column].fillna('').apply(preprocess_text)\n",
    "\n",
    "# Create the 'text' column by combining 'title', 'selftext', and 'body'\n",
    "combined_df['text'] = combined_df.apply(\n",
    "    lambda row: ' '.join(row[col] for col in ['title', 'selftext', 'body'] if col in row and pd.notnull(row[col])).strip(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop rows with empty 'text' after preprocessing\n",
    "preprocessed_df = combined_df[combined_df['text'] != '']\n",
    "\n",
    "# Save the preprocessed DataFrame to a new CSV file\n",
    "preprocessed_df.to_csv(preprocessed_csv_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed DataFrame saved to {preprocessed_csv_path}. Total rows: {len(preprocessed_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by: Score, Number of words, and Duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved to rpg/rpg_small_processed.csv.\n",
      "Total rows after filtering and removing duplicates: 2129078\n",
      "Trimmed rows saved to rpg/rpg_small_trimmed.csv.\n",
      "Total trimmed rows: 7901230\n",
      "Duplicate rows saved to rpg/rpg_small_duplicates.csv.\n",
      "Total number of rows with at least 2 duplicates: 8255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "preprocessed_csv_path = 'rpg/rpg_small_preprocessed.csv'\n",
    "processed_csv_path = 'rpg/rpg_small_processed.csv'\n",
    "trimmed_save = 'rpg/rpg_small_trimmed.csv'\n",
    "duplicate_save = 'rpg/rpg_small_duplicates.csv'\n",
    "\n",
    "# Define score, length, and duplicate thresholds\n",
    "score_threshold = 5\n",
    "length_threshold = 10\n",
    "duplicate_threshold = 2\n",
    "\n",
    "# Load the preprocessed DataFrame\n",
    "df = pd.read_csv(preprocessed_csv_path, low_memory=False)\n",
    "\n",
    "# Ensure text column is a string to avoid any unexpected errors during processing\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Save the original DataFrame before filtering\n",
    "df_original = df.copy()\n",
    "\n",
    "# Filter rows based on score and length thresholds\n",
    "if score_threshold is not None:\n",
    "    df = df[df['score'] >= score_threshold]\n",
    "if length_threshold is not None:\n",
    "    df = df[df['text'].str.len() >= length_threshold]\n",
    "\n",
    "# Group by the 'text' column and filter groups with more than the specified number of identical texts\n",
    "if duplicate_threshold is not None:\n",
    "    duplicates = df.groupby('text').filter(lambda x: len(x) > duplicate_threshold)\n",
    "else:\n",
    "    duplicates = pd.DataFrame()\n",
    "\n",
    "# Sort by 'text' to see the duplicates together\n",
    "duplicates_sorted = duplicates.sort_values(by='text')\n",
    "\n",
    "# Save the identified duplicates to a CSV for easier viewing\n",
    "duplicates_sorted.to_csv(duplicate_save, index=False)\n",
    "\n",
    "# Find the indices of rows to remove (duplicates)\n",
    "indices_to_remove = duplicates_sorted.index\n",
    "\n",
    "# Remove these rows from the DataFrame\n",
    "df = df.drop(indices_to_remove)\n",
    "\n",
    "# Save the trimmed rows to a separate file\n",
    "if score_threshold is not None and length_threshold is not None:\n",
    "    df_trimmed = df_original[(df_original['score'] < score_threshold) | (df_original['text'].str.len() < length_threshold)]\n",
    "elif score_threshold is not None:\n",
    "    df_trimmed = df_original[df_original['score'] < score_threshold]\n",
    "elif length_threshold is not None:\n",
    "    df_trimmed = df_original[df_original['text'].str.len() < length_threshold]\n",
    "else:\n",
    "    df_trimmed = pd.DataFrame()\n",
    "\n",
    "# Remove the duplicates from the trimmed rows\n",
    "if not df_trimmed.empty:\n",
    "    indices_to_remove_trimmed = indices_to_remove[indices_to_remove.isin(df_trimmed.index)]\n",
    "    df_trimmed = df_trimmed.drop(indices_to_remove_trimmed)\n",
    "\n",
    "df_trimmed.to_csv(trimmed_save, index=False)\n",
    "\n",
    "# Save the processed DataFrame\n",
    "df.to_csv(processed_csv_path, index=False)\n",
    "\n",
    "# Log information\n",
    "print(f\"Processed DataFrame saved to {processed_csv_path}.\")\n",
    "print(f\"Total rows after filtering and removing duplicates: {len(df)}\")\n",
    "print(f\"Trimmed rows saved to {trimmed_save}.\")\n",
    "print(f\"Total trimmed rows: {len(df_trimmed)}\")\n",
    "print(f\"Duplicate rows saved to {duplicate_save}.\")\n",
    "print(f\"Total number of rows with at least {duplicate_threshold} duplicates: {len(duplicates_sorted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Filtering by score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved to rpg/rpg_small_processed.csv.\n",
      "Total rows after filtering: 2137333\n",
      "Trimmed rows saved to rpg/rpg_small_trimmed.csv.\n",
      "Total trimmed rows: 7901230\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "preprocessed_csv_path = 'rpg/rpg_small_preprocessed.csv'\n",
    "processed_csv_path = 'rpg/rpg_small_processed.csv'\n",
    "trimmed_save = 'rpg/rpg_small_trimmed.csv'\n",
    "\n",
    "# Define score and length thresholds\n",
    "score_threshold = 5\n",
    "length_threshold = 10\n",
    "\n",
    "# Load the preprocessed DataFrame\n",
    "df = pd.read_csv(preprocessed_csv_path, low_memory=False)\n",
    "\n",
    "# Ensure text column is a string to avoid any unexpected errors during processing\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Save the original DataFrame before filtering\n",
    "df_original = df.copy()\n",
    "\n",
    "# Filter rows based on score and length thresholds\n",
    "if score_threshold is not None:\n",
    "    df = df[df['score'] >= score_threshold]\n",
    "if length_threshold is not None:\n",
    "    df = df[df['text'].str.len() >= length_threshold]\n",
    "\n",
    "# Save the processed DataFrame\n",
    "df.to_csv(processed_csv_path, index=False)\n",
    "\n",
    "# Save the trimmed rows to a separate file\n",
    "if score_threshold is not None and length_threshold is not None:\n",
    "    df_trimmed = df_original[(df_original['score'] < score_threshold) | (df_original['text'].str.len() < length_threshold)]\n",
    "elif score_threshold is not None:\n",
    "    df_trimmed = df_original[df_original['score'] < score_threshold]\n",
    "elif length_threshold is not None:\n",
    "    df_trimmed = df_original[df_original['text'].str.len() < length_threshold]\n",
    "else:\n",
    "    df_trimmed = pd.DataFrame()\n",
    "\n",
    "df_trimmed.to_csv(trimmed_save, index=False)\n",
    "\n",
    "# Log information\n",
    "print(f\"Processed DataFrame saved to {processed_csv_path}.\")\n",
    "print(f\"Total rows after filtering: {len(df)}\")\n",
    "print(f\"Trimmed rows saved to {trimmed_save}.\")\n",
    "print(f\"Total trimmed rows: {len(df_trimmed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare the preprocessing steps against the 'combined.csv' and the preprocessing rules (e.g., did we drop the right amount?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "consolidated_csv_path = 'G:/BERTopic/attachment/attach_combined.csv'\n",
    "preprocessed_csv_path = 'G:/BERTopic/attachment/attach_preprocessed.csv'\n",
    "\n",
    "# Load the original combined CSV\n",
    "combined_df = pd.read_csv(consolidated_csv_path, low_memory=False)\n",
    "\n",
    "# Load the preprocessed CSV\n",
    "preprocessed_df = pd.read_csv(preprocessed_csv_path, low_memory=False)\n",
    "\n",
    "# Patterns to search for in the text before any other preprocessing\n",
    "patterns_to_remove = r'\\[deleted\\]|\\[removed\\]|\\[deleted by user\\]|\\[removed by user\\]'\n",
    "\n",
    "# For each relevant column, check if it contains any of the keywords\n",
    "# and create a mask for rows that should be removed based on the original columns\n",
    "masks = []\n",
    "for column in ['body', 'title', 'selftext']:\n",
    "    if column in combined_df.columns:\n",
    "        mask = combined_df[column].str.contains(patterns_to_remove, na=False, regex=True)\n",
    "        masks.append(mask)\n",
    "\n",
    "# Combine masks with logical OR to find any rows that match the criteria in any text column\n",
    "combined_mask = pd.concat(masks, axis=1).any(axis=1)\n",
    "\n",
    "# Log removed rows based on the combined mask before preprocessing\n",
    "removed_rows_before = combined_df[combined_mask]\n",
    "\n",
    "# Review the removed rows to ensure they match expectations before preprocessing\n",
    "print(\"Sample of rows identified for removal based on original dataset:\")\n",
    "print(removed_rows_before.head())\n",
    "\n",
    "# After preprocessing, check again for any missed rows with the keywords in the 'text' column\n",
    "missed_rows_after = preprocessed_df['text'].str.contains(patterns_to_remove, na=False, regex=True)\n",
    "\n",
    "# Review any rows that were missed during preprocessing\n",
    "print(\"Sample of rows missed for removal in the preprocessed dataset:\")\n",
    "print(preprocessed_df[missed_rows_after].head())\n",
    "\n",
    "# Count and compare\n",
    "print(f\"Original rows: {len(combined_df)}\")\n",
    "print(f\"Rows after preprocessing: {len(preprocessed_df)}\")\n",
    "print(f\"Identified for removal before preprocessing: {len(removed_rows_before)}\")\n",
    "print(f\"Missed rows after preprocessing: {preprocessed_df[missed_rows_after].shape[0]}\")\n",
    "print(f\"Expected rows after preprocessing (Original - Identified): {len(combined_df) - len(removed_rows_before)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 5: Identify and remove identical posts/comments - weekly discussion threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved to rpg/rpg_small_deduplicated.csv.\n",
      "Total rows after removing duplicates: 2129078\n",
      "Total number of rows with at least two duplicates: 8255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "preprocessed_csv_path = 'rpg/rpg_small_processed.csv'\n",
    "duplicate_save = 'rpg/rpg_small_duplicates.csv'\n",
    "processed_csv_path = 'rpg/rpg_small_deduplicated.csv'\n",
    "\n",
    "# Load the preprocessed DataFrame\n",
    "df = pd.read_csv(preprocessed_csv_path, low_memory=False)\n",
    "\n",
    "# Ensure text column is a string to avoid any unexpected errors during processing\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Group by the 'text' column and filter groups with more than 2 identical texts\n",
    "duplicates = df.groupby('text').filter(lambda x: len(x) > 2)\n",
    "\n",
    "# Sort by 'text' to see the duplicates together\n",
    "duplicates_sorted = duplicates.sort_values(by='text')\n",
    "\n",
    "# Save the identified duplicates to a CSV for easier viewing\n",
    "duplicates_sorted.to_csv(duplicate_save, index=False)\n",
    "\n",
    "# Find the indices of rows to remove (duplicates)\n",
    "indices_to_remove = duplicates_sorted.index\n",
    "\n",
    "# Remove these rows from the original DataFrame\n",
    "df_cleaned = df.drop(indices_to_remove)\n",
    "\n",
    "# Save the cleaned DataFrame, now without the identified duplicates\n",
    "df_cleaned.to_csv(processed_csv_path, index=False)\n",
    "\n",
    "# Log information\n",
    "print(f\"Processed DataFrame saved to {processed_csv_path}.\")\n",
    "print(f\"Total rows after removing duplicates: {len(df_cleaned)}\")\n",
    "print(f\"Total number of rows with at least two duplicates: {len(duplicates_sorted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_check_csv(csv_file_path):\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Print the number of rows in the CSV\n",
    "        print(f\"Total number of rows: {len(df)}\")\n",
    "        \n",
    "        # Print the number of rows with 'deleted' or 'removed' in the 'text' column\n",
    "        deleted_removed_count = df['text'].str.contains('deleted|removed', na=False).sum()\n",
    "        print(f\"Number of rows with 'deleted' or 'removed' in the 'text' column: {deleted_removed_count}\")\n",
    "        \n",
    "        # Check for potential errors\n",
    "        # Example check: Null values in the 'text' column\n",
    "        null_values_count = df['text'].isnull().sum()\n",
    "        print(f\"Number of rows with null values in the 'text' column: {null_values_count}\")\n",
    "        \n",
    "        # Example check: Unexpected data types (non-string) in the 'text' column\n",
    "        non_string_count = df[df['text'].apply(lambda x: not isinstance(x, str))].shape[0]\n",
    "        print(f\"Number of rows with non-string values in the 'text' column: {non_string_count}\")\n",
    "        \n",
    "        # Add more checks as needed based on the expected data format\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function with the path to your CSV file\n",
    "load_and_check_csv(\"G:/BERTopic/attachment/attach_processed_length10.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Validating the dataset and checking datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATTEMPT ONE - Does not dcomputer similarity.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_csv_file.csv' with the actual file path\n",
    "csv_file_path = 'G:/BERTopic/attachment/attach_processed_length10.csv'\n",
    "df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "\n",
    "# Print each unique subreddit\n",
    "for subreddit in df['subreddit'].unique():\n",
    "    print(subreddit)\n",
    "\n",
    "# Print the data type of each column formatted as a table\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes.to_frame('Data Type'))  # Convert dtypes Series to DataFrame for nicer display\n",
    "\n",
    "# Display the first few rows of the 'text' column to get a sense of the content\n",
    "print(df['text'].head())\n",
    "\n",
    "# Drop rows containing 'deleted' or 'removed'\n",
    "df = df[~df['text'].str.contains(\"deleted|removed\", na=False)]\n",
    "\n",
    "# After removal, print the number of rows left for confirmation\n",
    "print(f\"Remaining rows after removing 'deleted' or 'removed': {len(df)}\")\n",
    "\n",
    "# Check for rows that might be empty or contain only spaces\n",
    "empty_or_spaces = df[df['text'].str.strip().eq(\"\")]\n",
    "print(f\"Number of empty or only spaces rows: {len(empty_or_spaces)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates a text and excel file that describes the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snake\\AppData\\Local\\Temp\\ipykernel_2112\\1805183992.py:19: DtypeWarning: Columns (2,4,5,8,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Configuration\n",
    "csv_file_path = 'rpg/rpg_small_preprocessed.csv'\n",
    "excel_file_path = 'rpg/analysis/rpg_small_descriptives.xlsx'\n",
    "plot_save_path = 'rpg/analysis/plots/'\n",
    "\n",
    "# Ensure the plot_save_path directory exists\n",
    "if not os.path.exists(plot_save_path):\n",
    "    os.makedirs(plot_save_path)\n",
    "\n",
    "# Load dataset without specifying dtype\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Compute 'word_count'\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Define word count ranges\n",
    "word_count_ranges = {\n",
    "    'less_than_10': (df['word_count'] < 10),\n",
    "    '10_to_49': ((df['word_count'] >= 10) & (df['word_count'] < 50)),\n",
    "    '50_to_99': ((df['word_count'] >= 50) & (df['word_count'] < 100)),\n",
    "    '100_to_199': ((df['word_count'] >= 100) & (df['word_count'] < 200)),\n",
    "    '200_or_more': (df['word_count'] >= 200)\n",
    "}\n",
    "\n",
    "# Define score ranges\n",
    "score_ranges = {\n",
    "    'less_than_0': (df['score'] < 0),\n",
    "    'equal_0': (df['score'] == 0),\n",
    "    '1_plus': (df['score'] > 0),\n",
    "    '5_plus': (df['score'] >= 5),\n",
    "    '10_plus': (df['score'] >= 10),\n",
    "    '20_plus': (df['score'] >= 20)\n",
    "}\n",
    "\n",
    "# Calculate the total number of rows for each word count range\n",
    "word_count_totals = {key: value.sum() for key, value in word_count_ranges.items()}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "word_count_df = pd.DataFrame(list(word_count_totals.items()), columns=['Range', 'Count'])\n",
    "\n",
    "# Function to generate plots\n",
    "def plot_distribution(data, title, xlabel, filename):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data, bins=50, kde=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Create a new workbook\n",
    "book = Workbook()\n",
    "\n",
    "# Create a 'Combined' sheet\n",
    "combined_sheet = book.create_sheet(title='Combined')\n",
    "\n",
    "# Generate and save plots for 'score' and 'word_count' for the entire dataset\n",
    "overall_score_filename = f\"{plot_save_path}overall_score.png\"\n",
    "plot_distribution(df['score'], \"Overall Score Distribution\", 'Score', overall_score_filename)\n",
    "overall_word_count_filename = f\"{plot_save_path}overall_word_count.png\"\n",
    "plot_distribution(df['word_count'], \"Overall Word Count Distribution\", 'Word Count', overall_word_count_filename)\n",
    "\n",
    "# Add images to the 'Combined' sheet at the desired location\n",
    "score_img = Image(overall_score_filename)\n",
    "word_count_img = Image(overall_word_count_filename)\n",
    "combined_sheet.add_image(score_img, 'B2')\n",
    "combined_sheet.add_image(word_count_img, 'B20')\n",
    "\n",
    "# Create a sheet for bucketed distributions of word count\n",
    "word_count_sheet = book.create_sheet(title='Word Count Distribution')\n",
    "for row in dataframe_to_rows(word_count_df, index=False, header=True):\n",
    "    word_count_sheet.append(row)\n",
    "word_count_plot_filename = f\"{plot_save_path}word_count_distribution.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Range', y='Count', data=word_count_df)\n",
    "plt.title(\"Word Count Distribution\")\n",
    "plt.xlabel(\"Word Count Range\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(word_count_plot_filename)\n",
    "plt.close()\n",
    "word_count_plot_img = Image(word_count_plot_filename)\n",
    "word_count_sheet.add_image(word_count_plot_img, 'B10')\n",
    "\n",
    "# Create a sheet for bucketed distributions of score\n",
    "score_sheet = book.create_sheet(title='Score Distribution')\n",
    "score_totals = {key: value.sum() for key, value in score_ranges.items()}\n",
    "score_df = pd.DataFrame(list(score_totals.items()), columns=['Range', 'Count'])\n",
    "for row in dataframe_to_rows(score_df, index=False, header=True):\n",
    "    score_sheet.append(row)\n",
    "score_plot_filename = f\"{plot_save_path}score_distribution.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Range', y='Count', data=score_df)\n",
    "plt.title(\"Score Distribution\")\n",
    "plt.xlabel(\"Score Range\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(score_plot_filename)\n",
    "plt.close()\n",
    "score_plot_img = Image(score_plot_filename)\n",
    "score_sheet.add_image(score_plot_img, 'B10')\n",
    "\n",
    "# Group the data by 'subreddit' and calculate the mean score and word count\n",
    "grouped_data = df.groupby('subreddit')[['score', 'word_count']].mean()\n",
    "\n",
    "# Create sheets for each subreddit\n",
    "for subreddit, data in grouped_data.iterrows():\n",
    "    sheet = book.create_sheet(title=subreddit)\n",
    "    subreddit_df = df[df['subreddit'] == subreddit]\n",
    "    \n",
    "    # Generate and save plots for 'score' and 'word_count' for the subreddit\n",
    "    subreddit_score_filename = f\"{plot_save_path}{subreddit}_score.png\"\n",
    "    plot_distribution(subreddit_df['score'], f\"{subreddit} Score Distribution\", 'Score', subreddit_score_filename)\n",
    "    subreddit_word_count_filename = f\"{plot_save_path}{subreddit}_word_count.png\"\n",
    "    plot_distribution(subreddit_df['word_count'], f\"{subreddit} Word Count Distribution\", 'Word Count', subreddit_word_count_filename)\n",
    "    \n",
    "    # Add images to the subreddit sheet at the desired location\n",
    "    subreddit_score_img = Image(subreddit_score_filename)\n",
    "    subreddit_word_count_img = Image(subreddit_word_count_filename)\n",
    "    sheet.add_image(subreddit_score_img, 'B2')\n",
    "    sheet.add_image(subreddit_word_count_img, 'B20')\n",
    "    \n",
    "    # Write descriptive statistics to the subreddit sheet\n",
    "    sheet['A1'] = 'Score'\n",
    "    sheet['A2'] = data['score']\n",
    "    sheet['A3'] = 'Word Count'\n",
    "    sheet['A4'] = data['word_count']\n",
    "\n",
    "# Remove the default sheet\n",
    "book.remove(book.active)\n",
    "\n",
    "# Save the workbook\n",
    "book.save(excel_file_path)\n",
    "\n",
    "# Close the workbook\n",
    "book.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
