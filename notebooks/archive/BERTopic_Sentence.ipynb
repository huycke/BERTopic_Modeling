{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first row from the first file and print the column labels\n",
    "df1 = pd.read_csv('G:/BERTopic/attachment/attach_sen2_sentencedata.csv', nrows=1)\n",
    "print(\"Column labels for 'attach_sen2_sentencedata.csv':\")\n",
    "print(df1.columns.tolist())\n",
    "\n",
    "# Load the first row from the second file and print the column labels\n",
    "df2 = pd.read_csv('G:/BERTopic/attachment/attach_processed_length10.csv', nrows=1)\n",
    "print(\"Column labels for 'attach_processed_length10.csv':\")\n",
    "print(df2.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA availability:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Active GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparation and Embedding Generation (To Be Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Set the embedding save path\n",
    "embeddings_path = 'G:/BERTopic/attachment/models/attach_sen2_embeddings.npy'\n",
    "\n",
    "# Path to save the dataframe\n",
    "dataframe_save_path = 'attachment/attach_sen2_sentencedata.csv'\n",
    "\n",
    "# Load your dataset\n",
    "data_path = 'G:/BERTopic/attachment/attach_processed_length10.csv'  # Update with your actual data path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure there's a unique ID for each document\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "\n",
    "# Tokenize documents into sentences and map sentences to their originating document ID\n",
    "all_sentences = []\n",
    "doc_id_for_each_sentence = []\n",
    "for _, row in df.iterrows():\n",
    "    sentences = sent_tokenize(row['text'])\n",
    "    all_sentences.extend(sentences)\n",
    "    doc_id_for_each_sentence.extend([str(row['id'])] * len(sentences))  ## CONVERTING TO STRING HERE\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "model = SentenceTransformer('thenlper/gte-large')\n",
    "embeddings = model.encode(all_sentences, show_progress_bar=True)\n",
    "\n",
    "np.save(embeddings_path, embeddings)\n",
    "\n",
    "# Save the 'all_sentences' and 'doc_id_for_each_sentence' to a dataframe and then to a CSV\n",
    "sentence_data = pd.DataFrame({\n",
    "    'all_sentences': all_sentences,\n",
    "    'doc_id_for_each_sentence': doc_id_for_each_sentence\n",
    "})\n",
    "sentence_data.to_csv(dataframe_save_path, index=False)\n",
    "\n",
    "print(f\"Embeddings saved to {embeddings_path}\")\n",
    "print(f\"Sentence data saved to {dataframe_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: BERTopic (To Be Run for Each Experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import collections\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "### Paths set in CODE BLOCK 1 ###\n",
    "embeddings_path = 'G:/BERTopic/attachment/models/sen2/attach_sen2_embeddings.npy'\n",
    "sentence_data_load_path = 'G:/BERTopic/attachment/models/sen2/attach_sen2_sentencedata.csv'  \n",
    "original_data_path = 'G:/BERTopic/attachment/attach_processed_length10.csv'  \n",
    "\n",
    "### Paths for saving results ###\n",
    "doc_topics_data_path = 'G:/BERTopic/attachment/attach_sen2_SenTopics.csv'\n",
    "model_save_path = 'G:/BERTopic/attachment/models/sen2/attach_sen2_model_dir'\n",
    "\n",
    "logging.info(\"Loading Embeddings\")\n",
    "# Load the embeddings\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "# Load the sentence data from the CSV\n",
    "sentence_data = pd.read_csv(sentence_data_load_path)  # Assuming this line exists in the actual code\n",
    "all_sentences = sentence_data['all_sentences'].tolist()  # Load all_sentences from the DataFrame\n",
    "all_sentences = [str(sentence) for sentence in all_sentences]  # Convert sentences to strings\n",
    "\n",
    "doc_id_for_each_sentence = sentence_data['doc_id_for_each_sentence'].tolist()\n",
    "\n",
    "logging.info(\"Loading Dataset\")\n",
    "# Load the original dataset\n",
    "df = pd.read_csv(original_data_path)\n",
    "\n",
    "# Ensure there's a 'docs' column if 'text' is used in CSV\n",
    "if 'docs' not in df.columns and 'text' in df.columns:\n",
    "    df.rename(columns={'text': 'docs'}, inplace=True)\n",
    "\n",
    "logging.info(\"Preparing CountVectorizer\")\n",
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer(ngram_range=(1, 3)).build_tokenizer()\n",
    "for doc in df['docs'].tolist():\n",
    "    vocab.update(tokenizer(doc))\n",
    "vocab = [word for word, frequency in vocab.items() if frequency >= 100]\n",
    "logging.info(f\"Vocabulary extracted. Size: {len(vocab)}\")\n",
    "\n",
    "logging.info(\"Preparing BERTopic model...\")\n",
    "# BERTopic model preparation\n",
    "embedding_model = SentenceTransformer('thenlper/gte-large')\n",
    "\n",
    "#UMAP parameters\n",
    "umap_model = UMAP(\n",
    "        n_components=5,  # has a wild impact hard to predict\n",
    "        n_neighbors=60,  # Higher is a more gloabl strcture\n",
    "        min_dist=0.01,   # Lower value means more dense packing\n",
    "        random_state=42, # Reproducability\n",
    "        metric=\"cosine\", # have to pick something\n",
    "        n_jobs=-1        # speed\n",
    "        )\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=200,           # smallest size group considered\n",
    "            min_samples=25,                 # larger is more conservative - more noise\n",
    "            leaf_size=40,                   # number of points per leaf node in the tree - default 40\n",
    "            gen_min_span_tree=True,         # True creates minimum spanning trees - increasing RAM\n",
    "            prediction_data=True,           # generates extra cached data of prediction labels for new data or reuse\n",
    "            cluster_selection_method='leaf', # eom is normal - leaf might get more homogeneous clusters\n",
    "            cluster_selection_epsilon=0.0,  # default - merges clusters below threshold\n",
    "            core_dist_n_jobs=-1,            # For speed\n",
    "            )\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# Using these in BERTopic\n",
    "topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "# After fitting the BERTopic model\n",
    "logging.info(\"Fitting BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(all_sentences, embeddings)\n",
    "\n",
    "# Putting stuff back into the DF for saving\n",
    "sentence_level_df = pd.DataFrame({\n",
    "    'sentence_id': range(len(all_sentences)),\n",
    "    'id': doc_id_for_each_sentence,\n",
    "    'topic_id': topics,\n",
    "    'probability': probs,\n",
    "    'sentence_text': all_sentences\n",
    "})\n",
    "\n",
    "# Merge to include additional columns from the original dataset\n",
    "merged_df = pd.merge(sentence_level_df, df[['id', 'link_id', 'author', 'created_utc', 'subreddit', 'score', 'author_flair_text']], on='id', how='left')\n",
    "\n",
    "# Save the merged DataFrame as a CSV\n",
    "merged_df.to_csv(doc_topics_data_path, index=False)  # Corrected path for saving results\n",
    "logging.info(\"Merged BERTopic sentence-level analysis data saved successfully as CSV.\")\n",
    "\n",
    "# Save the BERTopic model as a .safetensors file\n",
    "logging.info(\"Saving BERTopic model...\")\n",
    "topic_model.save(model_save_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "logging.info(\"BERTopic model saved successfully as a .safetensors file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing documents with outlier reduction - c-tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Step 1: Loading the model...\")\n",
    "# Load the BERTopic model\n",
    "topic_model = BERTopic.load('G:/BERTopic/attachment/models/attach_sen2_DocTopics_model_dir')\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Load the data\n",
    "doc_file = 'attachment/attach_processed_length10.csv'\n",
    "df = pd.read_csv(doc_file, usecols=['text', 'created_utc'], low_memory=False)\n",
    "\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "print(\"Step 3: Applying the model...\")\n",
    "# Apply the model to the documents\n",
    "topics, probs = topic_model.transform(docs)\n",
    "\n",
    "print(\"Step 4: Reducing outliers...\")\n",
    "# Use the \"c-TF-IDF\" strategy with a threshold\n",
    "new_topics, _ = topic_model.reduce_outliers(docs, topics, strategy=\"c-tf-idf\", threshold=0.1)\n",
    "\n",
    "# Reduce all outliers that are left with the \"distributions\" strategy\n",
    "final_topics, _ = topic_model.reduce_outliers(docs, new_topics, strategy=\"distributions\")\n",
    "\n",
    "print(\"Step 5: Updating the topic representations...\")\n",
    "# Update the topic representations with the new topics\n",
    "topic_model.update_topics(docs, topics=final_topics)\n",
    "\n",
    "print(\"Step 6: Performing topics over time analysis...\")\n",
    "# Perform topics over time analysis\n",
    "topics_over_time = topic_model.topics_over_time(docs, final_topics, timestamps, nr_bins=40)\n",
    "\n",
    "print(\"Step 7: Saving the results...\")\n",
    "# Save the topics and probabilities to the DataFrame\n",
    "df['Topic'] = final_topics\n",
    "df['Probability'] = probs\n",
    "\n",
    "# Save the DataFrame with the updated topics and probabilities\n",
    "df.to_excel('G:/BERTopic/attachment/analysis/attach_sen2_DocTopics.xlsx', index=False)\n",
    "\n",
    "# Save the topics over time results\n",
    "topics_over_time.to_excel('G:/BERTopic/attachment/analysis/attach_sen2_TopicsOverTime.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column labels after LIWC analysis:\n",
    "\n",
    "# ID\tTopic #\tDocument Text\tSubreddit\tProbability\tScore\tAuthor\tlink_id\tcreated_utc\tauthor_flair_text\tSegment\tWC\tAnalytic\tClout\tAuthentic\tTone\tWPS\tBigWords\tDic\tLinguistic\tfunction\tpronoun\tppron\ti\twe\tyou\tshehe\tthey\tipron\tdet\tarticle\tnumber\tprep\tauxverb\tadverb\tconj\tnegate\tverb\tadj\tquantity\tDrives\taffiliation\tachieve\tpower\tCognition\tallnone\tcogproc\tinsight\tcause\tdiscrep\ttentat\tcertitude\tdiffer\tmemory\tAffect\ttone_pos\ttone_neg\temotion\temo_pos\temo_neg\temo_anx\temo_anger\temo_sad\tswear\tSocial\tsocbehav\tprosocial\tpolite\tconflict\tmoral\tcomm\tsocrefs\tfamily\tfriend\tfemale\tmale\tCulture\tpolitic\tethnicity\ttech\tLifestyle\tleisure\thome\twork\tmoney\trelig\tPhysical\thealth\tillness\twellness\tmental\tsubstances\tsexual\tfood\tdeath\tneed\twant\tacquire\tlack\tfulfill\tfatigue\treward\trisk\tcuriosity\tallure\tPerception\tattention\tmotion\tspace\tvisual\tauditory\tfeeling\ttime\tfocuspast\tfocuspresent\tfocusfuture\tConversation\tnetspeak\tassent\tnonflu\tfiller\tAllPunc\tPeriod\tComma\tQMark\tExclam\tApostro\tOtherP\tEmoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Documents without Outlier Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASS DOCUMENTS WITHOUT USING AN OUTLIER REDUCTION STRATEGY\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Defined paths\n",
    "data_save_path = 'G:/BERTopic/attachment/attach_processed_length10.csv'\n",
    "model_save_path = 'G:/BERTopic/attachment/models/attach_sen1_model.pkl'\n",
    "\n",
    "# Load the documents and the BERTopic model\n",
    "df_docs = pd.read_csv(data_save_path)\n",
    "documents = df_docs['text'].tolist()\n",
    "bertopic_model = BERTopic.load(model_save_path)\n",
    "\n",
    "# Apply the model to the documents\n",
    "topics, probs = bertopic_model.transform(documents)\n",
    "\n",
    "# Create a DataFrame with the original document texts and their assigned topics\n",
    "df_topics = pd.DataFrame({'id': df_docs['id'], 'document': documents, 'topic': topics})\n",
    "\n",
    "# Determine if the row is a comment or submission based on the 'title' column\n",
    "df_docs['type'] = np.where(df_docs['title'].isna(), 'comment', 'submission')\n",
    "\n",
    "# Merge the type, subreddit, and author information with the topics DataFrame\n",
    "merged_df = pd.merge(df_topics, df_docs[['id', 'type', 'subreddit', 'author']], on='id', how='left')\n",
    "\n",
    "# Reorder and select the specified columns to match the final requirement\n",
    "final_df = merged_df[['id', 'document', 'type', 'subreddit', 'author', 'topic']]\n",
    "\n",
    "# Save the enriched DataFrame to an .xlsx file\n",
    "final_df.to_excel('G:/BERTopic/attachment/analysis/attach_sen1_doc_level_enriched.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Step 2: Loading the model...\")\n",
    "# Load the BERTopic model\n",
    "topic_model = BERTopic.load('G:/BERTopic/attachment/models/attach_sen2_DocTopics_model.pkl')\n",
    "\n",
    "# Load the DataFrame from the .pkl file\n",
    "with open('G:/BERTopic/attachment/models/attach_sen2_DocTopics_model.pkl', \"rb\") as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Load the data\n",
    "doc_file = ('attachment/attach_processed_length10.csv')\n",
    "df = pd.read_csv(data_path, usecols=['text', 'timestamps'], low_memory=False)\n",
    "\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "timestamps = df['timestamps'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "# Load the BERTopic model\n",
    "topic_model = BERTopic.load('G:/BERTopic/attachment/models/attach_sen2_DocTopics_model.pkl')\n",
    "\n",
    "# Load the data\n",
    "doc_file = ('attachment/attach_processed_length10.csv')\n",
    "df = pd.read_csv(data_path, usecols=['text', 'timestamps'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "timestamps = df['timestamps'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Extract the results\n",
    "topics = topic_model.get_topics()\n",
    "topic_freq = topic_model.get_topic_freq()\n",
    "topic_info = topic_model.get_topic_info()\n",
    "representative_docs = topic_model.get_representative_docs()\n",
    "\n",
    "txt_file_path = 'attachment/analysis/attach_sen2_DocTopics_analysis.txt'\n",
    "csv_file_path = 'attachment/analysis/attach_sen2_DocTopics_analysis.csv'  \n",
    "\n",
    "# Save the results in a more structured and readable manner\n",
    "with open(txt_file_path, 'w') as f:\n",
    "    # Topics\n",
    "    f.write(\"TOPICS:\\n\")\n",
    "    for topic_num, terms in topics.items():\n",
    "        terms_str = ', '.join([term[0] for term in terms])\n",
    "        f.write(f\"Topic {topic_num}: {terms_str}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Topic Frequency\n",
    "    f.write(\"TOPIC FREQUENCY:\\n\")\n",
    "    for index, row in topic_freq.iterrows():\n",
    "        f.write(f\"Topic {row['Topic']}: {row['Count']} entries\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # Representative Docs\n",
    "    f.write(\"REPRESENTATIVE DOCS:\\n\")\n",
    "    for topic_num, docs in representative_docs.items():\n",
    "        f.write(f\"Topic {topic_num} representative docs:\\n\")\n",
    "        for doc in docs:\n",
    "            f.write(f\"  - {doc}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Convert 'topic_info' DataFrame directly to CSV\n",
    "topic_info.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Documents from Target Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'G:/BERTopic/attachment/models/attach_sen1_doc_topics_liwc.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# Define the topics you're interested in\n",
    "topics = [14, 20, 51, 54, 83, 98, 102, 112, 116]\n",
    "\n",
    "# Filter documents based on the specified topics\n",
    "filtered_docs = df[df['Topic #'].isin(topics)]\n",
    "\n",
    "# Save the filtered documents to a new Excel file\n",
    "output_path = 'G:/BERTopic/attachment/analysis/attach_sen1_juanita_topics.xlsx'\n",
    "filtered_docs.to_excel(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'G:/BERTopic/attachment/models/attach_sen1_doc_topics_liwc.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# BERTopic model loading and specified topics\n",
    "topic_model = BERTopic.load(model_save_path)  # Ensure model_save_path is defined\n",
    "topics = [14, 20, 51, 54, 83, 98, 102, 112, 116]\n",
    "\n",
    "# Initialize a DataFrame for aggregated stats\n",
    "aggregated_stats_df = pd.DataFrame()\n",
    "\n",
    "# Retrieve representations and calculate mean statistics\n",
    "for topic in topics:\n",
    "    topic_representation = topic_model.get_topic(topic)\n",
    "    representation_str = ', '.join([f\"{word} ({score:.2f})\" for word, score in topic_representation])\n",
    "    \n",
    "    topic_docs = df[df['Topic #'] == topic]\n",
    "    mean_stats = topic_docs[stats_columns].mean().to_frame().T  # Compute mean\n",
    "    mean_stats['Topic #'] = topic\n",
    "    mean_stats['Representations'] = representation_str\n",
    "    aggregated_stats_df = pd.concat([aggregated_stats_df, mean_stats], axis=0)\n",
    "\n",
    "# Rearrange columns as specified for aggregated_stats_df\n",
    "columns_order = ['Topic #', 'Representations'] + stats_columns\n",
    "aggregated_stats_df = aggregated_stats_df[columns_order]\n",
    "\n",
    "# Filter documents and rearrange columns as specified for filtered_docs\n",
    "filtered_columns = ['ID', 'Topic #', 'Document Text', 'Subreddit', 'Probability', 'Score', 'Author', 'link_id', 'created_utc', 'author_flair_text', 'author_flair_richtext']\n",
    "filtered_docs = df[df['Topic #'].isin(topics)][filtered_columns]\n",
    "\n",
    "# Save to Excel with the two sheets\n",
    "output_path = 'G:/BERTopic/attachment/analysis/attach_sen1_juanita_topics.xlsx'\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    aggregated_stats_df.to_excel(writer, sheet_name='Descriptive Statistics', index=False)\n",
    "    filtered_docs.to_excel(writer, sheet_name='Filtered Documents', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
