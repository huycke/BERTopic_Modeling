{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snake\\miniconda3\\envs\\maybert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 11895/11895 [34:27<00:00,  5.75it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data_path = 'attachment/attach_processed_length10.csv'\n",
    "\n",
    "# Set embedding save path\n",
    "embeddings_path = 'attachment/models/doc/attach_doc_embeddings.npy'\n",
    "\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Prepare sub-models\n",
    "embedding_model = SentenceTransformer('thenlper/gte-large')\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "np.save(embeddings_path, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading the data {data_path}\n",
      "Step 2: Preparing the documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380624/380624 [00:09<00:00, 40563.78it/s]\n",
      "2024-05-05 22:10:29,620 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-05 22:17:26,630 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-05 22:17:26,635 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-05-05 22:17:39,393 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-05 22:17:39,432 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-05 22:18:03,771 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# Define file paths\n",
    "data_path = 'attachment/attach_processed_length10.csv'\n",
    "embeddings_path = 'attachment/models/doc/attach_doc_embeddings.npy'\n",
    "model_save_path = 'attachment/models/doc/attach_doc3_model_dir/'\n",
    "\n",
    "print(\"Step 1: Loading the data {data_path}\")\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "###### Extract vocab to be used in BERTopic\n",
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer(ngram_range=(1, 4)).build_tokenizer()\n",
    "for doc in tqdm(docs):\n",
    "    vocab.update(tokenizer(doc))\n",
    "vocab = [word for word, frequency in vocab.items() if frequency >= 30]; len(vocab)\n",
    "\n",
    "\n",
    "umap_model = UMAP(\n",
    "        n_components=3,  # has a wild impact hard to predict\n",
    "        n_neighbors=20,  # Higher is a more gloabl strcture\n",
    "        min_dist=0.01,   # Lower value means more dense packing\n",
    "        random_state=42, # Reproducability\n",
    "        metric=\"cosine\", # have to pick something\n",
    "        n_jobs=-1        # speed\n",
    "        )\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=100,           # smallest size group considered\n",
    "            min_samples=20,               # larger is more conservative - more noise\n",
    "            leaf_size=40,                   # number of points per leaf node in the tree - default 40\n",
    "            gen_min_span_tree=True,        # True creates minimum spanning trees - increasing RAM\n",
    "            prediction_data=True,           # generates extra cached data of prediction labels for new data or reuse\n",
    "            cluster_selection_method='eom', # eom is normal - leaf might get more homogeneous clusters\n",
    "            cluster_selection_epsilon=0.0,  # default - merges clusters below threshold\n",
    "            core_dist_n_jobs=-1,            # For speed\n",
    "            )\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "topic_model.save(model_save_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic   Count                                  Name  \\\n",
      "0       -1  210760                      -1_you_to_and_it   \n",
      "1        0   46044                         0_da_fa_ap_he   \n",
      "2        1   15655                      1_she_her_you_to   \n",
      "3        2   10252                      2_he_him_his_you   \n",
      "4        3    5939            3_text_texting_phone_texts   \n",
      "..     ...     ...                                   ...   \n",
      "209    208     104     208_change_changing_habits_people   \n",
      "210    209     102  209_thread_psychoanalyze_weekly_user   \n",
      "211    210     101  210_resentment_hurt_forgiveness_them   \n",
      "212    211     100        211_dating_single_myself_break   \n",
      "213    212     100           212_needs_need_what_needing   \n",
      "\n",
      "                                        Representation  \\\n",
      "0        [you, to, and, it, the, that, of, he, in, is]   \n",
      "1          [da, fa, ap, he, and, to, the, was, me, my]   \n",
      "2     [she, her, you, to, was, that, and, it, me, for]   \n",
      "3    [he, him, his, you, it, to, your, and, that, t...   \n",
      "4    [text, texting, phone, texts, day, reply, if, ...   \n",
      "..                                                 ...   \n",
      "209  [change, changing, habits, people, they, thems...   \n",
      "210  [thread, psychoanalyze, weekly, user, please, ...   \n",
      "211  [resentment, hurt, forgiveness, them, resentfu...   \n",
      "212  [dating, single, myself, break, date, on, been...   \n",
      "213  [needs, need, what, needing, littering, met, a...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [It feels that way now, and how you feel sad, ...  \n",
      "1    [This might be very long and kind of disorgani...  \n",
      "2    [Thank you for the kind words, it actually cal...  \n",
      "3    [I find the last two paragraphs that you wrote...  \n",
      "4    [I totally understand. Even if you knew they w...  \n",
      "..                                                 ...  \n",
      "209  [Sometimes people do change. They have to want...  \n",
      "210  [1. Relationship topics by any users asking ab...  \n",
      "211  [Sounds a bit like resentment, What are your t...  \n",
      "212  [Since when is deciding to not date \"the easy ...  \n",
      "213  [I think I don't know how to get my needs met....  \n",
      "\n",
      "[214 rows x 5 columns]\n",
      "DataFrame saved as 'topic_info.csv'\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the topic information as a DataFrame and assign it to topic_info_df\n",
    "topic_info_df = topic_model.get_topic_info()\n",
    "print(topic_info_df)\n",
    "\n",
    "# Now you can save topic_info_df as a CSV file\n",
    "topic_info_df.to_csv('attachment/analysis/doc/attach_doc3_topic_info.csv', index=False)\n",
    "print(\"DataFrame saved as 'topic_info.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a bunch of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading the data {data_path}\n",
      "Step 2: Preparing the documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380624/380624 [00:08<00:00, 42399.63it/s]\n",
      "2024-05-05 22:18:22,599 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-05 22:26:31,065 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-05 22:26:31,069 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-05-05 22:26:44,308 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-05 22:26:44,345 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-05 22:27:08,065 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic   Count                                               Name  \\\n",
      "0       -1  204313                                    -1_you_to_he_it   \n",
      "1        0   63760                                     0_da_fa_ap_and   \n",
      "2        1   14520                                   1_she_her_you_to   \n",
      "3        2    6215             2_thoughts_journaling_anxiety_emotions   \n",
      "4        3    5461                  3_avoidant_avoidants_anxious_they   \n",
      "..     ...     ...                                                ...   \n",
      "260    259      51                     259_stage_stages_mindset_thank   \n",
      "261    260      51  260_incompatibility_compatibility_incompatibil...   \n",
      "262    261      51             261_memories_childhood_remember_memory   \n",
      "263    262      50                  262_sabotage_self_completing_goal   \n",
      "264    263      50                  263_intimacy_intimate_ea_physical   \n",
      "\n",
      "                                        Representation  \\\n",
      "0       [you, to, he, it, and, that, the, of, in, for]   \n",
      "1          [da, fa, ap, and, he, the, to, was, of, my]   \n",
      "2      [she, her, you, to, that, was, and, it, me, if]   \n",
      "3    [thoughts, journaling, anxiety, emotions, help...   \n",
      "4    [avoidant, avoidants, anxious, they, relations...   \n",
      "..                                                 ...   \n",
      "260  [stage, stages, mindset, thank, yellow, 34, gu...   \n",
      "261  [incompatibility, compatibility, incompatibili...   \n",
      "262  [memories, childhood, remember, memory, events...   \n",
      "263  [sabotage, self, completing, goal, failure, bo...   \n",
      "264  [intimacy, intimate, ea, physical, vulnerable,...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [I relate to this so much. I've been in and ou...  \n",
      "1    [this girl brings out some AP in me, whereas w...  \n",
      "2    [How would I be able to tell the difference be...  \n",
      "3    [Hey, AP here too. I'm also on the journey tow...  \n",
      "4    [Hello. Could really use some insight here. I ...  \n",
      "..                                                 ...  \n",
      "260  [I love this!! I'm about at the end of stage 2...  \n",
      "261  [\"Compatibility isn't a rigid thing. It all ab...  \n",
      "262  [FA and childhood memories caring a week ago a...  \n",
      "263  [How would it be self sabotage? :S, Even if it...  \n",
      "264  [The things you have listed for physical intim...  \n",
      "\n",
      "[265 rows x 5 columns]\n",
      "DataFrame saved as 'topic_info.csv'\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# Define file paths\n",
    "data_path = 'attachment/attach_processed_length10.csv'\n",
    "embeddings_path = 'attachment/models/doc/attach_doc_embeddings.npy'\n",
    "model_save_path = 'attachment/models/doc/attach_doc4_model_dir/'\n",
    "\n",
    "print(\"Step 1: Loading the data {data_path}\")\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "###### Extract vocab to be used in BERTopic\n",
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer(ngram_range=(1, 3)).build_tokenizer()\n",
    "for doc in tqdm(docs):\n",
    "    vocab.update(tokenizer(doc))\n",
    "vocab = [word for word, frequency in vocab.items() if frequency >= 30]; len(vocab)\n",
    "\n",
    "\n",
    "umap_model = UMAP(\n",
    "        n_components=3,  # has a wild impact hard to predict\n",
    "        n_neighbors=30,  # Higher is a more gloabl strcture\n",
    "        min_dist=0.01,   # Lower value means more dense packing\n",
    "        random_state=42, # Reproducability\n",
    "        metric=\"cosine\", # have to pick something\n",
    "        n_jobs=-1        # speed\n",
    "        )\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=50,           # smallest size group considered\n",
    "            min_samples=30,               # larger is more conservative - more noise\n",
    "            leaf_size=40,                   # number of points per leaf node in the tree - default 40\n",
    "            gen_min_span_tree=True,        # True creates minimum spanning trees - increasing RAM\n",
    "            prediction_data=True,           # generates extra cached data of prediction labels for new data or reuse\n",
    "            cluster_selection_method='eom', # eom is normal - leaf might get more homogeneous clusters\n",
    "            cluster_selection_epsilon=0.0,  # default - merges clusters below threshold\n",
    "            core_dist_n_jobs=-1,            # For speed\n",
    "            )\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "topic_model.save(model_save_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "# Retrieve the topic information as a DataFrame and assign it to topic_info_df\n",
    "topic_info_df = topic_model.get_topic_info()\n",
    "print(topic_info_df)\n",
    "\n",
    "# Now you can save topic_info_df as a CSV file\n",
    "topic_info_df.to_csv('attachment/analysis/doc/attach_doc4_topic_info.csv', index=False)\n",
    "print(\"DataFrame saved as 'topic_info.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading the data {data_path}\n",
      "Step 2: Preparing the documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380624/380624 [00:08<00:00, 42750.09it/s]\n",
      "2024-05-05 22:27:25,036 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-05 22:36:54,043 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-05 22:36:54,048 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-05-05 22:37:06,619 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-05 22:37:06,656 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-05 22:37:29,906 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic   Count                                               Name  \\\n",
      "0       -1  222697                                   -1_you_to_and_it   \n",
      "1        0   42327                                      0_da_fa_ap_he   \n",
      "2        1   11854                                   1_she_her_you_to   \n",
      "3        2   10293                                   2_he_him_his_you   \n",
      "4        3    5420                3_thoughts_journaling_anxiety_helps   \n",
      "..     ...     ...                                                ...   \n",
      "257    256      52                   256_love_magnet_anxiety_yourself   \n",
      "258    257      52                         257_run_hills_fuck_running   \n",
      "259    258      52  258_introverted_introversion_introverts_introvert   \n",
      "260    259      51                            259_him_text_he_contact   \n",
      "261    260      50                    260_dodged_bullet_dodge_bullets   \n",
      "\n",
      "                                        Representation  \\\n",
      "0        [you, to, and, it, that, the, of, he, is, in]   \n",
      "1          [da, fa, ap, he, and, was, to, the, me, my]   \n",
      "2    [she, her, you, to, that, was, it, and, if, said]   \n",
      "3      [he, him, his, you, was, it, and, to, that, me]   \n",
      "4    [thoughts, journaling, anxiety, helps, emotion...   \n",
      "..                                                 ...   \n",
      "257  [love, magnet, anxiety, yourself, fear, myself...   \n",
      "258  [run, hills, fuck, running, awaaaaaaayyyyyyyy,...   \n",
      "259  [introverted, introversion, introverts, introv...   \n",
      "260  [him, text, he, contact, texted, again, month,...   \n",
      "261  [dodged, bullet, dodge, bullets, thenthey, hug...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [Sure! So first month of our relationship, no ...  \n",
      "1    [AP going through a break up with DA Hi all! I...  \n",
      "2    [She needed a lot of personal space and time a...  \n",
      "3    [Just to add a bit more insight... I think he ...  \n",
      "4    [It's definitely not easy and takes lots of se...  \n",
      "..                                                 ...  \n",
      "257  [Million times yes! This is exactly me and, li...  \n",
      "258  [You should run away then., DA here. RUN!, Run...  \n",
      "259  [Maybe there is some introversion as well who ...  \n",
      "260  [I wavered between sending him an upset text m...  \n",
      "261  [Dodged a bullet, You dodged a bullet., You do...  \n",
      "\n",
      "[262 rows x 5 columns]\n",
      "DataFrame saved as 'topic_info.csv'\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# Define file paths\n",
    "data_path = 'attachment/attach_processed_length10.csv'\n",
    "embeddings_path = 'attachment/models/doc/attach_doc_embeddings.npy'\n",
    "model_save_path = 'attachment/models/doc/attach_doc5_model_dir/'\n",
    "\n",
    "print(\"Step 1: Loading the data {data_path}\")\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "###### Extract vocab to be used in BERTopic\n",
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer(ngram_range=(1, 3)).build_tokenizer()\n",
    "for doc in tqdm(docs):\n",
    "    vocab.update(tokenizer(doc))\n",
    "vocab = [word for word, frequency in vocab.items() if frequency >= 30]; len(vocab)\n",
    "\n",
    "\n",
    "umap_model = UMAP(\n",
    "        n_components=3,  # has a wild impact hard to predict\n",
    "        n_neighbors=40,  # Higher is a more gloabl strcture\n",
    "        min_dist=0.01,   # Lower value means more dense packing\n",
    "        random_state=42, # Reproducability\n",
    "        metric=\"cosine\", # have to pick something\n",
    "        n_jobs=-1        # speed\n",
    "        )\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=50,           # smallest size group considered\n",
    "            min_samples=30,               # larger is more conservative - more noise\n",
    "            leaf_size=25,                   # number of points per leaf node in the tree - default 40\n",
    "            gen_min_span_tree=True,        # True creates minimum spanning trees - increasing RAM\n",
    "            prediction_data=True,           # generates extra cached data of prediction labels for new data or reuse\n",
    "            cluster_selection_method='eom', # eom is normal - leaf might get more homogeneous clusters\n",
    "            cluster_selection_epsilon=0.0,  # default - merges clusters below threshold\n",
    "            core_dist_n_jobs=-1,            # For speed\n",
    "            )\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "topic_model.save(model_save_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "# Retrieve the topic information as a DataFrame and assign it to topic_info_df\n",
    "topic_info_df = topic_model.get_topic_info()\n",
    "print(topic_info_df)\n",
    "\n",
    "# Now you can save topic_info_df as a CSV file\n",
    "topic_info_df.to_csv('attachment/analysis/doc/attach_doc5_topic_info.csv', index=False)\n",
    "print(\"DataFrame saved as 'topic_info.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading the data {data_path}\n",
      "Step 2: Preparing the documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380624/380624 [00:08<00:00, 42347.64it/s]\n",
      "2024-05-05 22:37:46,086 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-05 22:50:45,036 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-05 22:50:45,043 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-05-05 22:51:10,623 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-05 22:51:10,656 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-05 22:51:32,888 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   Count                              Name  \\\n",
      "0     -1     100                 -1_bot_you_the_to   \n",
      "1      0  379527                  0_to_and_the_you   \n",
      "2      1     703                 1_dog_and_cat_the   \n",
      "3      2     141       2_page_please_the_questions   \n",
      "4      3      85  3_bot_fucking_optout_shakespeare   \n",
      "5      4      68         4_boop_beep_comments_peek   \n",
      "\n",
      "                                      Representation  \\\n",
      "0     [bot, you, the, to, this, is, and, of, it, in]   \n",
      "1     [to, and, the, you, it, that, of, is, in, for]   \n",
      "2   [dog, and, cat, the, to, cats, my, it, pets, of]   \n",
      "3  [page, please, the, questions, sub, for, toran...   \n",
      "4  [bot, fucking, optout, shakespeare, thy, words...   \n",
      "5  [boop, beep, comments, peek, sneak, downvote, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [This is what anxious attachment looks like Ft...  \n",
      "1  [Yes you are gonna need be self aware of your ...  \n",
      "2  [I was worried about influencing my dog negati...  \n",
      "3  [Thank you for your post, AvenueLane96. Here a...  \n",
      "4  [\"i very much did enjoy spending time with the...  \n",
      "5  [Here's a sneak peek of rAvoidantAttachment us...  \n",
      "DataFrame saved as 'topic_info.csv'\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# Define file paths\n",
    "data_path = 'attachment/attach_processed_length10.csv'\n",
    "embeddings_path = 'attachment/models/doc/attach_doc_embeddings.npy'\n",
    "model_save_path = 'attachment/models/doc/attach_doc6_model_dir/'\n",
    "\n",
    "print(\"Step 1: Loading the data {data_path}\")\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, usecols=['text'], low_memory=False)\n",
    "\n",
    "print(\"Step 2: Preparing the documents...\")\n",
    "# Specify what the 'docs' are\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "###### Extract vocab to be used in BERTopic\n",
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer(ngram_range=(1, 3)).build_tokenizer()\n",
    "for doc in tqdm(docs):\n",
    "    vocab.update(tokenizer(doc))\n",
    "vocab = [word for word, frequency in vocab.items() if frequency >= 30]; len(vocab)\n",
    "\n",
    "\n",
    "umap_model = UMAP(\n",
    "        n_components=5,  # has a wild impact hard to predict\n",
    "        n_neighbors=60,  # Higher is a more gloabl strcture\n",
    "        min_dist=0.1,   # Lower value means more dense packing\n",
    "        random_state=42, # Reproducability\n",
    "        metric=\"cosine\", # have to pick something\n",
    "        n_jobs=-1        # speed\n",
    "        )\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=50,           # smallest size group considered\n",
    "            min_samples=30,               # larger is more conservative - more noise\n",
    "            leaf_size=50,                   # number of points per leaf node in the tree - default 40\n",
    "            gen_min_span_tree=True,        # True creates minimum spanning trees - increasing RAM\n",
    "            prediction_data=True,           # generates extra cached data of prediction labels for new data or reuse\n",
    "            cluster_selection_method='eom', # eom is normal - leaf might get more homogeneous clusters\n",
    "            cluster_selection_epsilon=0.0,  # default - merges clusters below threshold\n",
    "            core_dist_n_jobs=-1,            # For speed\n",
    "            )\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "topic_model.save(model_save_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "# Retrieve the topic information as a DataFrame and assign it to topic_info_df\n",
    "topic_info_df = topic_model.get_topic_info()\n",
    "print(topic_info_df)\n",
    "\n",
    "# Now you can save topic_info_df as a CSV file\n",
    "topic_info_df.to_csv('attachment/analysis/doc/attach_doc6_topic_info.csv', index=False)\n",
    "print(\"DataFrame saved as 'topic_info.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maybert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
